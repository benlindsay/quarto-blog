[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "I’m a data scientist in the Twin Cities. I got into data science through chemical engineering, so, you know, the usual way. My research wasn’t very conducive to data science, so I kind of made my own path. I co-founded Penn Data Science Group, applied as many good coding and data analysis practices as I could to my research, and ran a couple tutorials/workshops to share useful tools (like Jupyter, Pandas, and Git) in PDSG events and research lab meetings. I did a few side projects to pick up skills outside of research. Here are some of the projects I had the most fun with:\n\nPBCluster: This is the only one that actually had to do with my research. I put together a Python package that I don’t think anyone but me will use, but I had fun doing it and learning how to put together a tested, object-oriented, documented, pip-installable package that helped at least 1 person (me) with their research.\nCollaborative Filtering Methods Comparison: A detailed blog post comparing several collaborative filtering models for movie recommendation with the MovieLens dataset.\nCitadel Data Open Championship: My team’s report from the final round of a national datathon analyzing education data\nInteractive Baby Name Popularity Map: An interactive map made with D3.js that lets you explore baby name popularity by time and location.\nNFL Fantasy Draft Dashboard: A dashboard made with Plotly Dash just for fun to try to help with a 2018 NFL fantasy draft. Turned out to be useless, but it was fun putting it together.\n\nSo many people helped me in my career development and job search process leading up to landing an exciting data science job, so I’m always happy to (try to) pay it forward with advice, encouragement, or connections."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nProfiling Jupyter Notebook Code with py-spy\n\n\n\n\n\n\n\njupyter\n\n\npy-spy\n\n\nperformance\n\n\npython\n\n\n\n\nWhen you sync your Jupyter notebooks with Jupytext, you get to keep all the benefits of Jupyter notebooks while also being able to pass your code through a profiler like py-spy to get a rich, interactive visualization that helps you quickly understand where the bottlenecks are.\n\n\n\n\n\n\nMay 14, 2021\n\n\nBen Lindsay\n\n\n\n\n\n\n  \n\n\n\n\nComparing Collaborative Filtering Methods\n\n\n\n\n\n\n\nrecommender systems\n\n\ncollaborative filtering\n\n\npython\n\n\nnumpy\n\n\npandas\n\n\nseaborn\n\n\njupyter\n\n\nmatplotlib\n\n\n\n\nI wanted to dive into the fundamentals of collaborative filtering and recommender systems, so I implemented a few common methods and compared them.\n\n\n\n\n\n\nOct 14, 2019\n\n\nBen Lindsay\n\n\n\n\n\n\n  \n\n\n\n\nDeploying a Cookiecutter Django Site on AWS\n\n\n\n\n\n\n\naws\n\n\ndjango\n\n\ndocker\n\n\npython\n\n\n\n\nStep-by-step walkthrough of deploying a Django app to AWS using the Django Cookiecutter template.\n\n\n\n\n\n\nJul 23, 2019\n\n\nBen Lindsay\n\n\n\n\n\n\n  \n\n\n\n\nRunning Jupyter Lab Remotely\n\n\n\n\n\n\n\nbash\n\n\njupyter\n\n\ntmux\n\n\nssh\n\n\nproductivity\n\n\n\n\nI have a research cluster where I do most of my analyses for my PhD work, and running Jupyter Lab directly on the cluster means I don’t have to copy files between the cluster and my desktop.\n\n\n\n\n\n\nApr 30, 2018\n\n\nBen Lindsay\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyzing Larger-than-Memory Data on your Laptop\n\n\n\n\n\n\n\npython\n\n\nrecommender systems\n\n\ndask\n\n\npandas\n\n\nbig data\n\n\n\n\nDask is an amazing Python library that lets you do all your Pandas-style dataframe manipulations with just a few simple tweaks so you don’t have to worry about Jupyter freezing up.\n\n\n\n\n\n\nMar 10, 2017\n\n\nBen Lindsay\n\n\n\n\n\n\n  \n\n\n\n\nTaking Advantage of Sparsity in the ALS-WR Algorithm\n\n\n\n\n\n\n\npython\n\n\nmachine learning\n\n\ncollaborative filtering\n\n\nrecommender systems\n\n\n\n\nA little tweak to some code from a great tutorial speeds up computation by taking advantage of sparsity\n\n\n\n\n\n\nFeb 11, 2017\n\n\nBen Lindsay\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDealing with Grid Data in Python\n\n\n\n\n\n\n\npython\n\n\nnumpy\n\n\n\n\nSome tricks for dealing with 2D and 3D grid data in python.\n\n\n\n\n\n\nDec 8, 2016\n\n\nBen Lindsay\n\n\n\n\n\n\n  \n\n\n\n\nInteractive D3 Map of Baby Name Popularity\n\n\n\n\n\n\n\npython\n\n\nd3\n\n\ndata visualization\n\n\n\n\nPick a name and slide the slider to see how its popularity changed over time across the US\n\n\n\n\n\n\nJul 24, 2016\n\n\nBen Lindsay\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter Sweep Bash Script\n\n\n\n\n\n\n\nbash\n\n\nproductivity\n\n\n\n\n\n\n\n\n\n\n\nDec 19, 2015\n\n\nBen Lindsay\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/interactive-d3-map-of-baby-name-popularity/index.html",
    "href": "posts/interactive-d3-map-of-baby-name-popularity/index.html",
    "title": "Interactive D3 Map of Baby Name Popularity",
    "section": "",
    "text": "The map above is an iframe of baby-name-map.surge.sh, so the interactivity can be a little laggy. If you follow the link and play with it there, the map might respond faster when dragging the Year slider."
  },
  {
    "objectID": "posts/interactive-d3-map-of-baby-name-popularity/index.html#using-and-understanding-this-map",
    "href": "posts/interactive-d3-map-of-baby-name-popularity/index.html#using-and-understanding-this-map",
    "title": "Interactive D3 Map of Baby Name Popularity",
    "section": "Using and Understanding this Map",
    "text": "Using and Understanding this Map\nTo use the map above, select a name from the dropdown list (you should be able to type a name if you don’t want to scroll), then drag the slider to move in time between the years 1910 and 2014. The color hue (pink vs. blue) in each state tells you whether the name was more popular for baby girls or boys. The color tone (dark pink vs. light pink) corresponds to the name’s popularity among babies of that gender. “Popularity” is measured by the percentage of babies of either gender given that name.\nFor ideas on where to start, try out some of the more popular gender neutral names described in a FiveThirtyEight article such as Casey, Riley, Jessie, or Jackie. Or look at Jaime (also in that list) and see the popularity skyrocket in 1976, which turns out to be when The Bionic Woman aired on TV, portraying the adventures of a female cyborg spy named Jaime Sommers."
  },
  {
    "objectID": "posts/interactive-d3-map-of-baby-name-popularity/index.html#why-i-made-this",
    "href": "posts/interactive-d3-map-of-baby-name-popularity/index.html#why-i-made-this",
    "title": "Interactive D3 Map of Baby Name Popularity",
    "section": "Why I Made This",
    "text": "Why I Made This\n\nMy wife and I recently had a baby, so I’ve been interested in baby name trends and wanted an interactive way to visualize them.\nI wanted to learn how to use D3, and Mike Bostock’s Quantile Chloropleth example caught my eye.\n\nI’ll split my discussion of this project into 3 parts: the data prep part, the D3 part, and the playing with the map part."
  },
  {
    "objectID": "posts/interactive-d3-map-of-baby-name-popularity/index.html#the-data-prep-part",
    "href": "posts/interactive-d3-map-of-baby-name-popularity/index.html#the-data-prep-part",
    "title": "Interactive D3 Map of Baby Name Popularity",
    "section": "The Data Prep Part",
    "text": "The Data Prep Part\nI wanted the map to respond quickly to moving the slider for any given name, but I didn’t want the browser to have to load in too much data at a time, so I decided to make a separate file for each name which contained all the data relevant to that name. The raw data, which I downloaded from Kaggle, (data download link here) needed to be processed a bit before my map could use it. An IPython Notebook using the pandas Python module was a great tool for this purpose. You can see my Notebook on GitHub. (I’m pretty excited that GitHub now renders the contents of IPython Notebooks by the way.)\nI’ve been using Python (mostly numpy) for data analysis for a couple years now, but this was my first real introduction to pandas. I found the DataFrame.pivot_table() function particularly useful in this project. It allowed me to very easily create a dataframe with states as the rows and years as the columns out of a dataframe with a different row for each name."
  },
  {
    "objectID": "posts/interactive-d3-map-of-baby-name-popularity/index.html#the-d3-part",
    "href": "posts/interactive-d3-map-of-baby-name-popularity/index.html#the-d3-part",
    "title": "Interactive D3 Map of Baby Name Popularity",
    "section": "The D3 Part",
    "text": "The D3 Part\nCreating the map using D3 wasn’t as difficult as I expected. It was mostly a matter of following along with Mike Bostock’s choropleth example. Scott Murray’s D3 Tutorials were also incredibly useful. I highly recommend them for anyone interested in checking out D3. The Javascript code used to generate the map can be seen here."
  },
  {
    "objectID": "posts/interactive-d3-map-of-baby-name-popularity/index.html#the-playing-with-the-map-part",
    "href": "posts/interactive-d3-map-of-baby-name-popularity/index.html#the-playing-with-the-map-part",
    "title": "Interactive D3 Map of Baby Name Popularity",
    "section": "The Playing With the Map Part",
    "text": "The Playing With the Map Part\nThe gender neutral names make the most interesting visualizations, so I looked through FiveThirtyEight’s list of the most popular gender neutral names.\nCasey, the most popular one, is interesting because of how clean the male/female split was at some points. Here’s what it looked like in 1980:\n\n\n\nCasey in 1980\n\n\nAnother interesting one was Jaime. Here’s the map for Jaime in 1975:\n\n\n\nJaime in 1975\n\n\nJust one year later, when The Bionic Woman aired on TV, everyone must have loved Jaime, the female cyborg spy because suddenly Jaime’s popularity among babies skyrocketed. Here’s the map for 1976:\n\n\n\nJaime in 1976\n\n\nIf you see anything interesting with other names, go ahead and post below."
  },
  {
    "objectID": "posts/profiling-jupyter-code-with-py-spy/index.html",
    "href": "posts/profiling-jupyter-code-with-py-spy/index.html",
    "title": "Profiling Jupyter Notebook Code with py-spy",
    "section": "",
    "text": "Jupyter notebooks are great for interactive development and visualization, but it’s hard to get nice code profiling visualizations. Jake VanderPlas has a great exceprt in his Python Data Science Handbook with examples of how to time and profile code in a Jupyter notebook. These work great, but the text output isn’t always as intuitive as visualizations like flame graphs. They give you a hierarchical view of which lines of code at all depths of the call stack are contributing the most to the compute time. Not only that, they’re interactive SVGs! Click on the image in the flame graphs link and you can check out the interactivity yourself. You can zoom into specific parts of the call stack and search for terms, and hover over blocks to see the full description at the top. As far as I can tell, there are ways to get a line-by-line heat map in Jupyter, like this, but you don’t get the full call stack like in a flame graph.\n\n\n\nFlame Graph Example\n\n\nThere’s a fantastic package called py-spy that creates flame graphs, but that works best with .py files. Now we just need a way to get our Jupyter notebook represented as a .py file. There are a couple of options for this. One would be to export the file as a .py file from the File menu of your Jupyter server. Another option is to use Jupytext to create a synced .py file. The benefit of this is that if you make changes in the .py file, they’ll be reflected in the notebook, making it easier to switch between them.\nTo do this, first install Jupytext with\n$ pip install jupytext\nthen, if you already have a notebook called my_notebook.ipynb, create a synced pair of files with\n$ jupytext --set-formats ipynb,py:percent my_notebook.ipynb\nwhich will create and sync my_notebook.py. Go here a quick reference of some CLI commands available to Jupytext.\nFrom there, you can run py-spy on the my_notebook.py. First, install py-spy if you haven’t already:\n$ pip install py-spy\nThen execute the script with py-spy and output a flame graph to my_flame_graph.svg:\n$ py-spy record -o my_flame_graph.svg -- python my_notebook.py\nDrag that SVG file onto a web browser and you can explore your flame graph interactively.\n\nSome Caveats\nIf you have Jupyter magic commands (anything starting with %) or display() invocations, you’ll run into errors executing as a flat .py script. You’ll need to comment out the Jupyter magics, and add a from IPython.display import display if you have display() anywhere.\nPlease hit me up on Twitter or comment below if you found this useful, ran into issues with any of the steps in this post, or have suggestions about alternative ways to profile code in Jupyter notebooks."
  },
  {
    "objectID": "posts/comparing-collaborative-filtering-methods/index.html",
    "href": "posts/comparing-collaborative-filtering-methods/index.html",
    "title": "Comparing Collaborative Filtering Methods",
    "section": "",
    "text": "As part of a project sponsored by the data science team at Air Liquide here in Philadelphia, I’m diving deep into collaborative filtering algorithms. There are 2 main goals of this project:\nThe data I’m using comes from the GroupLens research group, which has been collecting movie ratings from volunteers since 1995 and has curated datasets of a variety of sizes. For simplicity, I’ll focus on the 100K dataset, the smallest one, to enable faster iteration.\nI split this project into several parts. Here’s a table of contents for you:"
  },
  {
    "objectID": "posts/comparing-collaborative-filtering-methods/index.html#exploratory-data-analysis",
    "href": "posts/comparing-collaborative-filtering-methods/index.html#exploratory-data-analysis",
    "title": "Comparing Collaborative Filtering Methods",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nCheck out the full notebook for this section here.\nBefore getting into any algorithm development, I wanted to get a picture of the data I was working with, so I asked the questions on my mind and tried to answer them with the data.\nWhat does the ratings distribution look like?\n\n\n\nRatings Distribution\n\n\nIt’s a little skewed to the positive side, with 4 being the most common rating. I guess that skew makes sense because people are more likely to watch stuff they would like than stuff they would hate.\nNext: how consistent are the ratings over time? If people as a whole get more positive or negative over time, that could complicate things. If their behavior doesn’t seem to change too much, we can make a simplifying assumption that time doesn’t matter and ignore time dependence.\n\n\n\nRatings Consistency\n\n\nLooks pretty consistent, so we’re going to make that simplifying assumption. Purely out of curiosity, how much do the number of users and movies change over time?\n\n\n\nUser and Movie Count\n\n\nThe amount of growth in the short timespan of this dataset, particularly in the number of users, does make me think a more complicated approach could be warranted. Buuuuuut I don’t want to do that right now. We’ll stick with assuming we’re working with an IID dataset for the purposes of this project.\nA very crucial aspect to understand about typical recommendation situations is the sparsity of your dataset. You want to predict how much every user likes every movie, but we have data about very few user-movie combinations. We’ll explore this in two ways.\nFirst we’ll visualize the sparsity pattern of the user-movie matrix. This could be done with Matplotlib’s spy function, but I didn’t know about it at the time I did this analysis, so I did this manually. The plot below shows a single, tiny black square for every user/movie combination we have. If everyone rated every movie, you’d see a solid black rectangle. Instead what we see is a lot of white–lots of user/movie combinations for which we don’t have a rating (yet). You especially see a lot of white in the top right corner. This is probably because early raters had access to fewer movies to rate, and new users progressively had more movies to rate as they were added to the system.\n\n\n\nMovieLens Sparsity Map\n\n\nThe matrix density is \\(n_{ratings}/(n_{users}×n_{movies})=0.063\\), meaning that about 94% of the data we would like to know is missing.\nIn the plot above you also notice that there are a few darker rows and columns, but most rows and columns are pretty bare. Let’s visualize the distributions of number of ratings by user and by movie. The way I chose to visualize this is with an Empirical Cumulative Distribution Function (ECDF) plot. An ECDF plot has an advantage compared to a histogram that all data points can be plotted in a meaningful way, and no bin size has to be chosen to average arbitrary chunks of it. This is especially helpful with the long-tailed distributions here.\n\n\n\nECDF Plot\n\n\nIn the plot above, you can learn, for example, that 40% of all users rated 50 or less movies, and 90% of movies have 169 or less ratings. In general, we seen that a large fraction of movies and users have few ratings associated with them, but a few movies and users have many more ratings.\nThe main thing to take from this though is that the matrix of possible ratings is quite sparse, and that we need to use models that deal with this lack of data."
  },
  {
    "objectID": "posts/comparing-collaborative-filtering-methods/index.html#baseline-algorithms",
    "href": "posts/comparing-collaborative-filtering-methods/index.html#baseline-algorithms",
    "title": "Comparing Collaborative Filtering Methods",
    "section": "Baseline Algorithms",
    "text": "Baseline Algorithms\nCheck out the full notebook for this section here.\nBaseline models are important for 2 key reaons:\n\nBaseline models give us a starting point to which to compare all future models, and\nSmart baselines/averages may be needed to fill in missing data for more complicated models\n\nIn this section, we’ll explore a few typical baseline models for recommender systems and see which ones do the best for our dataset. For all of these baseline models, and for that matter all the “real” models in the following sections, I coded them with the following structure, roughly similar to Scikit-learn’s API:\nclass SomeBaselineModel():\n\n    def __init__(self):\n        # Run initialization steps\n\n    def fit(self, X):\n        # Compute model parameters from ratings dataframe X with user, movie,\n        # and rating columns\n        ...\n        return self\n\n    def predict(self, X):\n        # Predict ratings for dataframe X with user and movie columns\n        ...\n        return predictions\nI won’t actually put the code for all the models in here, but it’s all there in the Jupyter notebook.\n\nSimple Average Model\nThe first model I implemented is about the simplest one possible, which I called SimpleAverageModel. We’ll average all the training set ratings and use that average for the prediction for all test set examples. It probably won’t do very well, but hey, it’s a baseline!\n\n\nAverage By ID Model\nWe can probably do a little better by using the user or item (movie) average. To do this, I set up a baseline model class, which I called AverageByIdModel, that allows you to pass either a list of userIds or movieIds as X. The prediction for a given ID will be the average of ratings from that ID, or the overall average if that ID wasn’t seen in the training set. This will probably get us a little farther than SimpleAverageModel but it still won’t win any million-dollar prizes.\n\n\nDamped User + Movie Baseline\nLastly, we can likely do even better by taking into account average user and movie data for a given user-movie combo. It has an additional feature of a damping factor that can regularize the baseline prediction to prevent us from straying too far from that average of 4. The damping factor has been shown empirically to improve the baseline’s perfomance. I called my implementation DampedUserMovieBaselineModel.\nThis model follows equation 2.1 from a collaborative filtering paper from GroupLens, the same group that published the MovieLens data. This equation defines rhe baseline rating for user \\(u\\) and item \\(i\\) as\n\\[b_{u,i} = \\mu + b_u + b_i\\]\nwhere\n\\[b_u = \\frac{1}{|I_u| + \\beta_u}\\sum_{i \\in I_u} (r_{u,i} - \\mu)\\]\nand\n\\[b_i = \\frac{1}{|U_i| + \\beta_i}\\sum_{u \\in U_i} (r_{u,i} - b_u - \\mu).\\]\n(See equations 2.4 and 2.5). Here, \\(\\beta_u\\) and \\(\\beta_i\\) are damping factors, for which the paper reported 25 is a good number for this dataset. For now we’ll just leave these values equal (\\(\\beta=\\beta_u=\\beta_i\\)). Here’s a summary of the meanings of all the variables here:\n\n\n\n\n\n\n\nTerm\nMeaning\n\n\n\n\n\\(b_{u,i}\\)\nBaseline rating for user \\(u\\) on item (movie) \\(i\\)\n\n\n\\(\\mu\\)\nThe mean of all ratings\n\n\n\\(b_u\\)\nThe deviation from \\(\\mu\\) associated with user \\(u\\)\n\n\n\\(b_i\\)\nThe deviation from \\(\\mu+b_u\\) associated with user \\(i\\)\n\n\n\\(I_u\\)\nThe set of all items rated by user \\(u\\)\n\n\n\\(\\mid I_u \\mid\\)\nThe number of items rated by user \\(u\\)\n\n\n\\(\\beta_u\\)\nDamping factor for the users (\\(=\\beta\\))\n\n\n\\(r_{u,i}\\)\nObserved rating for user \\(u\\) on item \\(i\\)\n\n\n\\(U_i\\)\nThe set of all users who rated item \\(i\\)\n\n\n\\(\\mid U_i \\mid\\)\nThe number of users who rated item \\(i\\)\n\n\n\\(\\beta_i\\)\nDamping factor for the items (\\(=\\beta\\))\n\n\n\n\n\nBaseline Comparison\nWith those baseline models defined, let’s compare them. In the plot below, I test 7 baseline models. The first is the SimpleAverageModel. The next two use the AverageByIdModel looking at averages by Item ID and User ID, respectively. The last 4 use the DampedUserMovieBaseline with different damping factors (\\(\\beta\\)). The top plot shows the Mean Absolute Error (MAE) of each fold after using 5-fold cross-validation. I chose MAE so as not to overly penalize more extreme ratings (compared to Mean Squared Error) from people angrily or over-excitedly selecting 1 or 5. The bottom plot shows the distributions of the corresponding residuals, meaning the difference between actual and predicted ratings.\n\n\n\nBaseline Comparison\n\n\nThe MAE plots above show that the combined model with a damping factor of 0 or 10 performs the best, followed by the item average, then the user average. It makes sense that taking into account deviations from the mean due to both user and item would perform the best: there are more degrees of freedom (\\(n_{users}+n_{movies}\\) to be exact) being taken into account for each baseline prediction. The same idea explains why the item average performs better than the user average: there are more items than users in this dataset, so averaging over items gives you \\(n_{movies}\\) degrees of freedom, which is greater than the \\(n_{users}\\) degrees of freedom for the user average. The residual plots underneath the MAE plot illustrate that taking into account more data pulls the density of the residuals closer to 0.\nBefore moving on to collaborative filtering models, we’ll want to choose which model to use as a baseline. Both the Combined 0 and Combined 10 models performed equally well, but we’ll choose the Combined 10 model, because a higher damping factor is effectively stronger regularization, which will prevent overfitting better than a damping factor of 0."
  },
  {
    "objectID": "posts/comparing-collaborative-filtering-methods/index.html#similarity-based-algorithms",
    "href": "posts/comparing-collaborative-filtering-methods/index.html#similarity-based-algorithms",
    "title": "Comparing Collaborative Filtering Methods",
    "section": "Similarity-Based Algorithms",
    "text": "Similarity-Based Algorithms\nCheck out the full notebook for this section here.\nNow that we’ve established some simple baseline models and demonstrated that the Damped User + Movie Baseline model is the best of the few we tested, let’s move on to some actual collaborative filtering models. Here, we’ll explore user-based and item-based collaborative filtering.\n Image by Salem Marafi, found at salemmarafi.com\nThe idea of these methods is to predict unseen ratings by looking at how similar users rated a particular item, or by looking at how similar items were rated by a particular user. Both methods fall under the category of K-Nearest Neighbor (KNN) models, since ratings from the \\(k\\) most similar users or items are combined for the prediction.\nIn the notebook linked above, I’ve implemented a class called KNNRecommender that can accept a mode parameter of either 'user' or 'item'. In the plot below, I use 5-fold cross-validation to measure the MAE of user- and item-based models as a function of \\(k\\). The green band represents the mean \\(\\pm\\) standard deviation of the best baseline method chosen from the previous section.\n\n\n\nKNN k cross validation\n\n\nHere we can see that Item-based collaborative filtering outperforms User-based collaborative filtering for all \\(k\\). This occurs for the same reason that the Item average baseline performed better than the User average baseline: there are generally more ratings per item than there are ratings per user, since there are more users than movies. (This reverse is true for larger datasets like the MovieLens 20M Dataset where there are more users than movies.)\nWe also see that the best Item-based CF model occurs around \\(k=10\\) while the best User-based CF model occurs around \\(k=20\\). We’ll keep these in mind when comparing models later.\nNext, we’ll start looking at matrix factorization methods, beginning with Alternating Least Squares."
  },
  {
    "objectID": "posts/comparing-collaborative-filtering-methods/index.html#alternating-least-squares",
    "href": "posts/comparing-collaborative-filtering-methods/index.html#alternating-least-squares",
    "title": "Comparing Collaborative Filtering Methods",
    "section": "Alternating Least Squares",
    "text": "Alternating Least Squares\nCheck out the full notebook for this section here.\nPreviously, I showed how to use similarity-based approaches that guess unknown user-movie-rating triplets by looking at either movies with a similar rating profile or users with a similar rating profile. These approaches leave a lot of data on the table though. Matrix factorization is a way to both take into account more data and perform some regularizing dimensionality reduction to help deal with the sparsity problem.\nThe basic idea is to organize the user-movie-rating triplets into a matrix with each row representing a user and each column representing a movie. We want to approximate this large matrix with a matrix multiplication of 2 smaller matrices. In the example below, each row of the “User Matrix” has 2 latent features of that user, and each column of the “Item Matrix” has 2 latent features of that item. The dot product of any user’s latent features and item’s latent features will give an estimate of the rating that user would give that movie.\n Image by Soumya Gosh, found at medium.com\nThere are many variations on this theme and multiple ways to perform this matrix factorization. The method I demonstrate here is called “Alternating Least Squares” method which was designed for the Netflix Prize and described in this paper. This method works iteratively, with 2 main steps per iteration:\n\nAssume the User Matrix is fixed and solve for the Item Matrix\nAssume the Item Matrix is fixed and solve for the User Matrix\n\nIn the notebook linked above, the full code for the ALSRecommender can be found.\nSince this is an iterative method, I first checked the amount of iterations/epochs for an arbitrary number of latent features \\(k\\) before the error curves start to plateau:\n\n\n\nALS Epochs\n\n\nSo it looks like 15 or 20 epochs should be enough for Test Error to start plateauing. So now let’s stick with 15 epochs and use cross-validation to select an optimal \\(k\\):\n\n\n\nALS k Cross-Validation\n\n\nIt looks like we have a Test Error minimum around \\(k=5\\), so we’ll call that the winner for the ALS category.\nGreat, so now let’s move on to a different matrix factorization approach: stochastic gradient descent."
  },
  {
    "objectID": "posts/comparing-collaborative-filtering-methods/index.html#stochastic-gradient-descent",
    "href": "posts/comparing-collaborative-filtering-methods/index.html#stochastic-gradient-descent",
    "title": "Comparing Collaborative Filtering Methods",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nCheck out the full notebook for this section here.\nPreviously, I showed how to do matrix factorization using Alternating Least Squares (ALS). Now we’ll attempt to factorize the matrix into the same mathematical form, but we’ll use a different technique to get there.\nDerivation details that give us the update equations we need can be found here. I’ll just give the start and finish here.\nWe start with a loss function that looks like this:\n\\[\nL = \\sum_{u,i}(r_{ui} - \\hat{r}_{ui})^2\n  + \\lambda_{b_u} \\sum_u \\lVert b_u \\lVert^2\n  + \\lambda_{b_i} \\sum_i \\lVert b_i \\lVert^2 \\\\\n  + \\lambda_{x_u} \\sum_u \\lVert \\mathbf{x}_u \\lVert^2\n  + \\lambda_{y_i} \\sum_i \\lVert \\mathbf{y}_i \\lVert^2\n\\]\nThe first term is a sum of squared errors on the predicted rating, while all the other terms are regularizing penalties on too high of values, tunable by the 4 \\(\\lambda\\) parameters. \\(\\hat{r}_{ui}\\), the predicted rating for user \\(u\\) on item \\(i\\), is given by\n\\[\n\\hat{r}_{ui} = \\mu + b_u + b_i + \\mathbf{x}_u^\\top \\cdot \\mathbf{y}_i\n\\]\nWith this setup, we can iterate over ratings, compute the gradient in the loss function for that point with respect to each parameter \\(b_u\\), \\(b_i\\), \\(\\mathbf{x}_u\\), and \\(\\mathbf{y}_i\\). As mentioned in the post linked above, the final update equations look like this\n\\[\n%% MathJax doesn't support multiline equations, so I'm using a hack to get them to\n%% render correctly, pulled from\n%% https://github.com/mathjax/MathJax/issues/2312#issuecomment-538185951\n\\displaylines{\nb_u^{t+1} = b_u^{t} + \\eta (e_{ui} - \\lambda_{b_u})b_u \\\\\nb_i^{t+1} = b_i^{t} + \\eta (e_{ui} - \\lambda_{b_i})b_i \\\\\n\\mathbf{x}_u^{t+1} = \\mathbf{x}_u^{t} + \\eta (e_{ui} \\mathbf{y}_i - \\lambda_{x_u} \\mathbf{x}_u) \\\\\n\\mathbf{y}_i^{t+1} = \\mathbf{y}_i^{t} + \\eta (e_{ui} \\mathbf{x}_u - \\lambda_{y_i} \\mathbf{y}_i) \\\\\n}\n\\]\nwhere \\(\\eta\\) is the learning rate (a parameter that controls the speed of descent down the gradients) and \\(e_{ui}\\) is the prediction error given by \\(\\hat{r}_{ui} - r_{ui}\\).\nThe code for the SGDRecommender and the tuning of that model can be found in the notebook linked above.\nFirst, just like we did with ALS, let’s see how the testing error changes as this iterative model progresses:\n\n\n\nSGD Epochs\n\n\nIt looks like around 12 is the optimal number of iterations to run before we start overfitting, so we’ll use that from here on out. Next, just like before, let’s use cross-validation to find the best \\(k\\):\n\n\n\nSGD k Cross-Validation\n\n\nHonestly, the fact that training error came back up at \\(k=50\\) probably means I didn’t use the right amount of iterations/epochs, because training error should always go down with increasing model complexity. But my implementation of SGD is pretty slow and painful, and I really don’t want to rerun this. Using Cython or some other method to move the large amount of for looping into the C-layer could significantly reduce this pain, but I’m not getting into that right now.\nWith that caveat in mind, since \\(k=50\\) resulted in the lowest test error, we’ll declare that the winner of the SGD variants and move on to comparing all the models."
  },
  {
    "objectID": "posts/comparing-collaborative-filtering-methods/index.html#algorithm-comparisons",
    "href": "posts/comparing-collaborative-filtering-methods/index.html#algorithm-comparisons",
    "title": "Comparing Collaborative Filtering Methods",
    "section": "Algorithm Comparisons",
    "text": "Algorithm Comparisons\nCheck out the full notebook for this section here.\nNow that I’ve implemented 3 main classes of collaborative filtering methods (similarity-based, alternating least squares (ALS), and stochastic gradient descent (SGD)), it’s time to see how they stack up to each other.\nTo compare models, I’ll use 2 different metrics: mean absolute error (MAE) and normalized discounted cumulative gain (NDCG). MAE measures about how many stars off all the predictions are on average. This is useful information, but in most recommendation situations, the user will only see a few of the top recommendations given to them. The NDCG score tells us how “good” the top few recommendations are, with decreasing weight given the farther you go down the list.\nUsually, NDCG will be reported for a certain number of recommendations. If we just care about the first 3 recommendations, we would compute NDCG@3. If there were no movies that the user would have rated more highly than these 3, then NDCG@3 is 1.0. Lower values mean other movies would have gotten higher ratings.\nIf you’re interested, the math looks like this:\nGiven a vector \\(\\mathbf{r}\\) of \\(k\\) recommendations from most to least recommended, discounted cumulative gain (DCG) is given by:\n\\[DCG@k = \\sum_{i=1}^k \\frac{r_i}{\\log_2(i+1)}\\]\nNormalized DCG (NDCG) is DCG divided by the maximum possible DCG:\n\\[ NDCG@k = \\frac{DCG@k}{\\max_{\\mathbf{r}} DCG@k}\\]\nFirst let’s choose the best User-based model:\n\n\n\nMAE and NDCG for User-based\n\n\nNDCG@3 peaks at k=50, and MAE is pretty similar between k=20 to 100, so k=50 is the winner. Now let’s do the same thing for an item-based recommender:\n\n\n\nMAE and NDCG for Item-based\n\n\nHere, \\(k=10\\) and \\(k=20\\) have similar MAE and NDCG@3, we’ll favor higher \\(k\\) in nearest neigbor methods because higher \\(k\\) is less prone to overfitting. \\(k=20\\) is the winner of the item-based models.\nNow with the iterative ALS and SGD models, we haven’t yet seen how NDCG@3 changes over time, so we need to examine that first before doing tuning the \\(k\\) parameter.\n\n\n\nMAE and NDCG vs Epoch for ALS-based\n\n\n15 epochs still looks good for ALS, so let’s do our \\(k\\) tuning, sticking with 15 iterations:\n\n\n\nMAE and NDCG vs k for ALS-based\n\n\nHere, it looks like MAE is pretty flat with respect to the learning rate \\(\\lambda\\), but NDCG@3 shows some interesting variations. The highest NDCG@3 comes from \\(\\lambda=0.1\\) and \\(k>=50\\). With matrix factorization methods like ALS, we want to favor lower \\(k\\) for better generalizability, so \\(\\lambda=0.1\\) and \\(k=50\\) is the winner of the ALS category.\nHow does NDCG@3 change over time with the SGD model?\n\n\n\nMAE and NDCG vs Epoch for SGD-based\n\n\nOof, looks like we’re going to need more than 15 epochs to get both the MAE and NDCG@3 to plateau, but I’m not redoing that plot because time is money and my slow implementation of SGD is sure costing a lot of time. I’ll ramp up to 30 iterations for model tuning hope that’s good enough. Now with SGD there are a lot more parameters you could tune. For the sake of time, we’ll stick with \\(k=50\\) based on the ALS results, and tune the learning rate (\\(\\eta\\)) and regularization parameters (\\(\\lambda_*\\)). We’re going to further simplify things by forcing all the regularization parameters to be equal and call them \\(\\lambda\\), i.e.\n\\[\\lambda_{b_u}=\\lambda_{b_i}=\\lambda_{x_u}=\\lambda_{y_i}=\\lambda\\]\nHere’s are the errors and NDCG@3 as a function of \\(\\lambda\\) and \\(\\eta\\):\n\n\n\nMAE and NDCG for SGD model tuning\n\n\n\\(\\lambda=\\eta=0.01\\) gives the best combination of MAE and NDCG@3, so that combination is the winner for SGD.\n\n\n\nModel Comparison\n\n\nThere’s a lot of information in the 3 charts above. The charts show 3 different metrics (Mean Absolute Error, Normalized Discounted Cumulative Gain, and time) for the best user-based, item-based, ALS, and SGD models I found. Each metric/model combination has 3 points, representing the values for each of the 3 folds used for cross-validation.\nThe MAE doesn’t seem to change much across the different models, although the variance seems to be slightly smaller for the matrix factorization methods (ALS and SGD) compared to the nearest neighbors methods (user-based and item-based).\nThe NDCG@3 does seem to vary across the different models though, with the highest score going to the ALS model. NDCG@3 is arguably the more useful metric for a recommender system, so as long as very high speeds aren’t important, ALS wins here.\nIf this ALS model is too slow for a particular application, the item-based method would be the next choice. Both user- and item-based recommenders have similarly fast training speeds, with item-based having a slightly higher NDCG@3 score. The slower execution of the ALS and SGD models are likely related to the number of iterations over for loops required in each iteration.\nAs they are right now, my user- and item-based models don’t need any python for loops during training. ALS has \\(n_{users} + n_{movies}\\) python for loop iterations per epoch, and SGD has \\(n_{ratings}\\) iterations per epoch, which is about an order of magnitude higher. I specify “python” for loops, because the vectorized operations used in user-based, item-based, and ALS models have for loops in c which are much faster than those in python. By optimizing code with something like cython or numba, I could certainly drop the training time for ALS and SGD."
  },
  {
    "objectID": "posts/comparing-collaborative-filtering-methods/index.html#recommender-system-prototype",
    "href": "posts/comparing-collaborative-filtering-methods/index.html#recommender-system-prototype",
    "title": "Comparing Collaborative Filtering Methods",
    "section": "Recommender System Prototype",
    "text": "Recommender System Prototype\nIf you want to play with these models interactively, check out my recommender notebook. With this notebook, you could choose whichever user you want, show some of their favorite movies, then display the top recommendations given by any of these 4 models.\nBelow is a screenshot of what you’ll see if you use this notebook. You input a user id (user 30 in this case), and the notebook displays posters of 5 of that user’s most highly rated movies. You choose a model ('als' in this case–other options are 'user', 'item', or 'sgd'), and the notebook takes a little time to compute, then displays the 3 movies the model most strongly recommends that user should watch. It’s not perfect, and I haven’t really verified that the results make sense, but it does something, so enjoy, and feel free to ask any questions below.\n\n\n\nMovies the input user likes\n\n\n\n\n\nMovies the system recommends"
  },
  {
    "objectID": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html",
    "href": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html",
    "title": "Analyzing Larger-than-Memory Data on your Laptop",
    "section": "",
    "text": "If you want to run some analysis on a dataset that’s just a little too big to load into memory on your laptop, but you don’t want to leave the comfort of using Pandas dataframes in a Jupyter notebook, then Dask may be just your thing. Dask is an amazing Python library that lets you do all your Pandas-style dataframe manipulations with just a few simple tweaks so you don’t have to worry about Jupyter freezing up.\nI’ll demonstrate the benefits of Dask and some of its syntax by running a calculation on business reviews provided for the Yelp Dataset Challenge, which contains 3.6 million business reviews. The reviews were provided in a file where each line is a JSON object with keys that include \"business_id\", \"user_id\", \"review_id\", \"stars\", and others. I extracted about 90% of all the JSON objects associated with businesses in Champaign, Illinois to one file as a small dataset that can be loaded into Pandas, and about 90% of all the JSON objects associated with any US/Canada business into another file as a larger dataset that does not fit into a Pandas dataframe on my laptop. You can view the notebook with all the code below here on GitHub."
  },
  {
    "objectID": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#baseline-prediction-method",
    "href": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#baseline-prediction-method",
    "title": "Analyzing Larger-than-Memory Data on your Laptop",
    "section": "Baseline Prediction Method",
    "text": "Baseline Prediction Method\nThe baseline prediction method I’ll show below is one of 4 methods discussed in this excellent survey of collaborative filtering recommender systems by Michael Ekstrand, John Riedl, and Joseph Konstan. The methods are:\n\nPredict by user’s average rating\nPredict by item’s average rating (“items” are businesses in this case)\nPredict by user’s and item’s average ratings\nPredict by user’s and item’s average ratings with damping factors\n\nThe 4th method ended up giving the best predictions on both the Champaign data and US/Canada training set. The damping factors reduce the weight placed on users or items with few reviews, making the prediction more robust. The necessary equations are 2.1, 2.4, and 2.5 in the survey linked above.\nEquation 2.1 (\\(b_{u,i} = \\mu + b_u + b_i\\)) essentially says that if we want the baseline prediction for user \\(u\\)’s rating of item \\(i\\), we can sum up the total average \\(\\mu\\), the offset from the \\(\\mu\\) corresponding to user \\(u\\) (\\(b_u\\)), and the offset from \\(\\mu + b_u\\) corresponding to item \\(i\\) (\\(b_i\\)).\nThe equations for \\(b_u\\) and \\(b_i\\) are\n\\[b_u = \\frac{1}{|I_u| + \\beta_u}\\sum_{i \\in I_u} (r_{u,i} - \\mu)\\]\n\\[b_i = \\frac{1}{|U_i| + \\beta_i}\\sum_{u \\in U_i} (r_{u,i} - b_u - \\mu)\\]\nwhere \\(r_{u,i}\\) is the actual rating of item (business) \\(i\\) given by user \\(u\\), \\(I_u\\) is the set of items rated by user \\(u\\), and \\(U_i\\) is the set of users who rated business \\(i\\)."
  },
  {
    "objectID": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#loading-data",
    "href": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#loading-data",
    "title": "Analyzing Larger-than-Memory Data on your Laptop",
    "section": "Loading Data",
    "text": "Loading Data\nFor all the following code blocks, assume we have the following imports:\nimport numpy as np\nimport pandas as pd\nimport dask.bag as db\nFirst, let’s compare the data loading process for the small and large datasets. In both cases, the data are in the form of a single file with one line of JSON data for each review. Loading the Champaign data using Pandas looks like this:\ndf_rev = pd.read_json('../preprocessed-data/all-champaign-reviews.json', orient='records', lines=True)\ndf_rev_champaign = df_rev_champaign[['review_id', 'business_id', 'user_id', 'stars']]\nFor the larger US/Canada training set, loading the data using Dask looks like this:\ndict_bag = db.read_text('../preprocessed-data/reviews_train.json', blocksize=int(5e6)).map(json.loads)\ndf_rev = dict_bag.to_dataframe(columns=['review_id', 'business_id', 'user_id', 'stars'])\ndf_rev = df_rev.repartition(npartitions=10)\nWhen loading in larger-than-memory data, Dask splits the data into partitions no larger than blocksize. You want to ensure you have enough partitions to ensure your computer doesn’t freeze, but too many will slow down the computation. For that reason, after I make a dataframe from a small subset of the features I read in, I repartition the data to reduce the number of partitions to 10. After the data are loaded in, you can treat your Dask datafame just like a Pandas dataframe (for the most part)."
  },
  {
    "objectID": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#computing-prediction-error",
    "href": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#computing-prediction-error",
    "title": "Analyzing Larger-than-Memory Data on your Laptop",
    "section": "Computing Prediction Error",
    "text": "Computing Prediction Error\nFor these baseline tests, I use the root mean squared error (RMSE) to measure the baseline accuracy. When dealing with Pandas dataframes, I can use a function like this:\ndef rmse_pandas(y_true, y_pred):\n    diff_sq = (y_true - y_pred) ** 2\n    return np.sqrt(diff_sq.mean())\nIn Dask, I can do the same thing with just an extra .compute() added, like so:\ndef rmse_dask(y_true, y_pred):\n    diff_sq = (y_true - y_pred) ** 2\n    return np.sqrt(diff_sq.mean().compute())\nThis is necessary because Dask uses “lazy evaluation” by default, and only computes results when you tell it to."
  },
  {
    "objectID": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#splitting-dataframe-into-train-and-test-sets",
    "href": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#splitting-dataframe-into-train-and-test-sets",
    "title": "Analyzing Larger-than-Memory Data on your Laptop",
    "section": "Splitting Dataframe into Train and Test Sets",
    "text": "Splitting Dataframe into Train and Test Sets\nSplitting the Pandas dataframe:\nfrom sklearn.model_selection import train_test_split\ndf_train_champaign, df_test_champaign = train_test_split(df_rev_champaign, random_state=0, test_size=0.2)\nSplitting the Dask dataframe:\ndf_train, df_test = df_rev.random_split([0.8, 0.2], random_state=0)\nUnfortunately we can’t use Scikit-learn on Dask dataframes, but a lot of the essential capabilities of Scikit-learn are implemented in Dask, or Dask compatible libraries."
  },
  {
    "objectID": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#computing-baselines",
    "href": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#computing-baselines",
    "title": "Analyzing Larger-than-Memory Data on your Laptop",
    "section": "Computing Baselines",
    "text": "Computing Baselines\nNow here’s the exciting part: the actual baseline computation uses the exact same code no matter whether it’s a Dask or Pandas dataframe. Here’s the function that computes the baseline predictions:\ndef compute_baseline_rmse(df_train, df_test, beta_u, beta_i, rmse_func):\n    \"\"\"\n    df_train and df_test are either Pandas or Dask dataframes\n    that must contain the columns 'user_id', 'business_id', and 'stars'.\n    beta_u and beta_i are user and business damping factors, respectively.\n    rmse_func is a function that computes the RMSE of the prediction\n    and takes Pandas or Dask Series objects, depending on whether\n    df_train and df_test are Pandas or Dask Dataframes.\n    \"\"\"\n    # Get mean rating of all training ratings\n    train_mean = df_train['stars'].mean()\n    # Get dataframe of b_u part of baseline for each user id\n    user_group = df_train[['user_id', 'stars']].groupby('user_id')\n    df_train_user = user_group.agg(['sum', 'count'])['stars']\n    df_train_user['b_u'] = (df_train_user['sum'] - train_mean * df_train_user['count'])\n    df_train_user['b_u'] /= (df_train_user['count'] + beta_u)\n    # Create column of b_u values corresponding to the user who made the review\n    df_train = df_train.join(df_train_user[['b_u']], on='user_id')\n    # Add column representing the expression inside the summation part of the b_i equation\n    df_train['b_i_sum'] = df_train['stars'] - df_train['b_u'] - train_mean\n    # Average over each business to get the actual b_i values for each business\n    bus_group = df_train[['business_id', 'b_i_sum']].groupby('business_id')\n    df_train_bus = bus_group.agg(['sum', 'count'])['b_i_sum'].rename(columns={'sum': 'b_i'})\n    df_train_bus['b_i'] /= df_train_bus['count'] + beta_i\n    # Join b_u and b_i columns to test dataframe\n    df_test = df_test.join(df_train_user[['b_u']], on='user_id').fillna(df_train_user['b_u'].mean())\n    df_test = df_test.join(df_train_bus[['b_i']], on='business_id').fillna(df_train_bus['b_i'].mean())\n    # Predict and Compute error\n    df_test['pred'] = df_test['b_u'] + df_test['b_i'] + train_mean\n    error = rmse_func(df_test['stars'], df_test['pred'])\n    print('Error = {}'.format(error))\nI call that function using either\ncompute_baseline_rmse(df_train_champaign, df_test_champaign, 5, 5, rmse_pandas)\nfor the Champaign Pandas dataframes or\ncompute_baseline_rmse(df_train, df_test, 5, 5, rmse_dask)\nfor the US/Canada Dask dataframes. Note that even relatively simple calculations like these can still take a long time if you’re just running on your laptop, especially if you more partitions than necessary."
  },
  {
    "objectID": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#conclusion",
    "href": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#conclusion",
    "title": "Analyzing Larger-than-Memory Data on your Laptop",
    "section": "Conclusion",
    "text": "Conclusion\nIf you want to do dataframe manipulations or standard machine learning on a dataset that’s just a little bigger than the memory you have available, I highly recommend Dask. For more complex computations or bigger datasets, you might want to stick with something fancier like Spark clusters in the cloud."
  },
  {
    "objectID": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#acknowledgments",
    "href": "posts/analyzing-larger-than-memory-data-on-your-laptop/index.html#acknowledgments",
    "title": "Analyzing Larger-than-Memory Data on your Laptop",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThanks to Ariel Rodriquez for introducing me to Dask, and thanks to Claire Zhang for finding the survey of collaborative filtering systems."
  },
  {
    "objectID": "posts/dealing-with-grid-data-in-python/index.html",
    "href": "posts/dealing-with-grid-data-in-python/index.html",
    "title": "Dealing with Grid Data in Python",
    "section": "",
    "text": "In my PhD research, I do a lot of analysis of 2D and 3D grid data output by simulations I run. If, for example, I had a toy system that was 3x2x2 grid points, the raw data would be structured sort of like this:\nx   y   z   value\n0   0   0   0.9\n1   0   0   1.1\n2   0   0   0.8\n0   1   0   1.1\n1   1   0   1.0\n2   1   0   0.9\n0   0   1   0.6\n1   0   1   1.2\n2   0   1   0.8\n0   1   1   0.9\n1   1   1   1.2\n2   1   1   1.3\nIn my analyses, it’s very helpful to restructure these data into a format where, in this case, x = [0, 1, 2], y = [0, 1], z = [0, 1], and value is a 3D array such that value[i, j, k] returns the value corresponding to position (x[i], y[j], z[k]).\nIt’s easy to do that in just a few lines. Say the above raw data is stored in data.dat.\n>>> import numpy as np\n>>> x, y, z, value = np.loadtxt('data.dat', skiprows=1).T\n>>> x, y, z = np.unique(x), np.unique(y), np.unique(z)\n>>> nx, ny, nz = len(x), len(y), len(z)\n>>> value = value.reshape((nz, ny, nx)).T\nNote that if the raw data had x varying the slowest and z varying the fastest, the final line would look like value = value.reshape((nx, ny, nz)).\nFinally, if you want to go the other way, where you have your x, y, and z arrays and 3D values array, you can make use of the sklearn.utils.extmath.cartesian function (first introduced on this StackOverflow post. If you want z to be the fastest changing variable, it would look something like this:\n>>> from sklearn.utils.extmath import cartesian\n>>> import numpy as np\n>>> x, y, z = [0, 1, 2], [0, 1], [0, 1]\n>>> nx, ny, nz = len(x), len(y), len(z)\n>>> value = np.arange(nx*ny*nz).reshape((nx,ny,nz)) # define 3D value array\n>>> xyz = cartesian((z, y, x))\n>>> value = value.flatten()\n>>> np.hstack((xyz, value[:,None]))\narray([[ 0,  0,  0,  0],\n        [ 0,  0,  1,  1],\n        [ 0,  1,  0,  2],\n        [ 0,  1,  1,  3],\n        [ 1,  0,  0,  4],\n        [ 1,  0,  1,  5],\n        [ 1,  1,  0,  6],\n        [ 1,  1,  1,  7],\n        [ 2,  0,  0,  8],\n        [ 2,  0,  1,  9],\n        [ 2,  1,  0, 10],\n        [ 2,  1,  1, 11]])\nThe value[:,None] thing on the last line adds an extra dimension to the 1D value array so the elements of the tuple passed to np.hstack are both 2D numpy arrays.\n>>> value.shape\n(12,)\n>>> value[:,None].shape\n(12, 1)\n>>> value[None,:].shape\n(1, 12)"
  },
  {
    "objectID": "posts/taking-advantage-of-sparsity-in-als-wr-algorithm/index.html",
    "href": "posts/taking-advantage-of-sparsity-in-als-wr-algorithm/index.html",
    "title": "Taking Advantage of Sparsity in the ALS-WR Algorithm",
    "section": "",
    "text": "I was interested in learning how to put together a recommender system for fun and practice. Since the Alternating-Least-Squares with Weighted-\\(\\lambda\\)-Regularization (ALS-WR) algorithm seems to be a popular algorithm for recommender systems, I decided to give it a shot. It was developed for the Netflix Prize competition, which also involved a sparse matrix of reviewers by items being reviewed.\nWhile searching for resources on the ALS-WR algorithm, I came across an excellent tutorial (whose link is now broken) that walks you through the theory and how to implement the algorithm using python on a small dataset of movie reviews. It even provided a link to download a Jupyter Notebook that you can run and see the algorithm in action. Having this notebook to toy around with was extremely helpful in familiarizing myself with the algorithm. However, as I compared the code in the notebook to the math in the blog post and in the original paper, it seemed like it wasn’t taking full advantage of the sparsity of the ratings matrix \\(R\\), which is a key feature of this type of problem. By slightly changing a couple lines in this code, I was able to dramatically reduce the computation time by taking advantage of the sparsity."
  },
  {
    "objectID": "posts/taking-advantage-of-sparsity-in-als-wr-algorithm/index.html#the-model",
    "href": "posts/taking-advantage-of-sparsity-in-als-wr-algorithm/index.html#the-model",
    "title": "Taking Advantage of Sparsity in the ALS-WR Algorithm",
    "section": "The Model",
    "text": "The Model\nI won’t walk through all the details because the notebok already does that really well, but I’ll give enough background to explain the change I made and why it speeds up the computation.\nWe start with a matrix \\(R\\) of size \\((m \\times n)\\) where each row represents one of the \\(m\\) users and each column represents one of the \\(n\\) movies. Most of the matrix contains 0’s since most users only review a small subset of the available movies. The dataset used in the tutorial contains only about 6% nonzero values. We want to generate a low-rank approximation for \\(R\\) such that \\(R \\approx P^TQ\\), where \\(P^T\\) is size \\((m \\times k)\\) and \\(Q\\) is size \\((k \\times n)\\), as shown below (image borrowed from the tutorial):\n\n\n\nALS-WR Matrix Schematic\n\n\nThe columns of the resulting matrices \\(P\\) and \\(Q\\) turn out to contain columns with \\(k\\) latent features about the users and movies, respectively. The \\(P\\) and \\(Q\\) matrices are calculated iteratively, by fixing one and solving for the other, then repeating while alternating which one is fixed. As a side note, in case you want to look at the paper, the notation is a little different. They use \\(U\\) and \\(M\\) instead of \\(P\\) and \\(Q\\), and \\(n_u\\) and \\(n_m\\) instead of \\(m\\) and \\(n\\). I’ll stick with the tutorial notation in this post.\nThe equations for solving for \\(P\\) and \\(Q\\) are quite similar, so let’s just look at the equation for \\(P\\). In each iteration, the column for each user in \\(P\\) is generated with the following equation:\n\\(\\mathbf{p}_i = A_i^{-1} V_i\\) where \\(A_i = Q_{I_i} Q_{I_i}^T + \\lambda n_{p_i} E\\) and \\(V_i = Q_{I_i} R^T(i, I_i)\\)\nHere, \\(E\\) is the \\((k \\times k)\\) identity matrix, \\(n_{p_i}\\) is the number of movies rated by user \\(i\\), and \\(I_i\\) is the set of all movies rated by user \\(i\\). That \\(I_i\\) in \\(Q_{I_i}\\) and \\(R(i, I_i)\\) means we are selecting only the columns for movies rated by user \\(i\\), and the way that selection is made makes all the difference."
  },
  {
    "objectID": "posts/taking-advantage-of-sparsity-in-als-wr-algorithm/index.html#selecting-columns",
    "href": "posts/taking-advantage-of-sparsity-in-als-wr-algorithm/index.html#selecting-columns",
    "title": "Taking Advantage of Sparsity in the ALS-WR Algorithm",
    "section": "Selecting Columns",
    "text": "Selecting Columns\nIn the tutorial, the key lines to generate each \\(\\mathbf{p}_i\\) look like this:\nAi = np.dot(Q, np.dot(np.diag(Ii), Q.T)) + lmbda * nui * E\nVi = np.dot(Q, np.dot(np.diag(Ii), R[i].T))\nP[:,i] = np.linalg.solve(Ai,Vi)\nNotice that in the equation for \\(A_i\\), the way it removes columns for movies that weren’t reviewed by user \\(i\\) is creating a \\((n \\times n)\\) matrix with the elements of \\(I_i\\) along the diagonal, then doing a \\((n \\times n) \\times (n \\times k)\\) matrix multiplication between that and \\(Q^T\\), which zeroes out columns of \\(Q\\) for movies user \\(i\\) did not review. This matrix multiplication is an expensive operation that (naively) has a complexity of \\(O(kn^2)\\) (although probably a bit better with the numpy implementation). A similar operation is done in the \\(V_i\\) calculation. Even though this is not as expensive (complexity of \\(O(n^2)\\)), that’s still an operation we’d like to avoid if possible.\nOn reading the equations and Matlab algorithm implementation in the original paper, I noticed that rather than zeroing out unwanted columns, they actually remove those columns by creating a submatrix of \\(Q\\) and a subvector of \\(\\mathbf{r}_i\\). This does 2 important things: First, it lets us remove that inner matrix multiplications. Second, it dramatically reduces the cost of the remaining matrix multiplications. Since we have a density of only about 6% in our \\(R\\) matrix, the cost of both \\(Q_{I_i}Q_{I_i}^T\\) and \\(Q_{I_i}R^T(i,I_i)\\) should theoretically be reduced to about 6% of their original costs, since the complexities of those operations (\\(O(nk^2)\\) and \\(O(nk)\\)) are both linearly dependent on \\(n\\). Here’s the code that replaces the 3 lines shown above:\n# Get array of nonzero indices in row Ii\nIi_nonzero = np.nonzero(Ii)[0]\n# Select subset of Q associated with movies reviewed by user i\nQ_Ii = Q[:, Ii_nonzero]\n# Select subset of row R_i associated with movies reviewed by user i\nR_Ii = R[i, Ii_nonzero]\nAi = np.dot(Q_Ii, Q_Ii.T) + lmbda * nui * E\nVi = np.dot(Q_Ii, R_Ii.T)\nP[:, i] = np.linalg.solve(Ai, Vi)\nBy making that replacement and a similar one for the equations for \\(\\mathbf{q}_j\\), a series of 15 iterations went from taking 15-16 minutes down to about 13 seconds: a ~70-fold speedup! Check out the notebook with my updates on GitHub, or clone the whole repo to run it yourself."
  },
  {
    "objectID": "posts/taking-advantage-of-sparsity-in-als-wr-algorithm/index.html#conclusions",
    "href": "posts/taking-advantage-of-sparsity-in-als-wr-algorithm/index.html#conclusions",
    "title": "Taking Advantage of Sparsity in the ALS-WR Algorithm",
    "section": "Conclusions",
    "text": "Conclusions\nThe moral of the story here is that sometimes things that don’t seem like a big deal at first glance can make huge changes in the performance of your algorithms. This exercise reinforced in my mind the value of spending a little extra time to make sure you understand the algorithm or tool you’re using. And more specifically, if you have a sparse dataset, make that sparsity work for you."
  },
  {
    "objectID": "posts/parameter-sweep-bash-script/index.html",
    "href": "posts/parameter-sweep-bash-script/index.html",
    "title": "Parameter Sweep Bash Script",
    "section": "",
    "text": "In my polymer simulation research, often my studies involve running a bunch of simulations where I pick one or more input parameters and change them over a range of values, then compare the results of each separate simulation to see how that/those variable(s) affect the system I’m simulating. This kind of study is called a “parameter sweep”, and can also be referred to as “embarrassingly parallel”, because the processor(s) for each for each individual job don’t need to communicate with the processor(s) from any other job. It can be very tedious to manually create input files for each job, so I wrote a bash script to help me out.\nFor example, if I want to simulate 3 different polymer nanocomposite systems, each with a different nanorod length, I could manually create 3 directories like so:\nmkdir length1\nmkdir length2\nmkdir length3\nThen I could copy an input file, bcp.input, and a submit file, sub.sh into each of those folders like so:\nfor d in length*/ ; do\n    cp bcp.input \"$d\"\n    cp sub.sh \"$d\" \ndone\nThen I could proceed to manually edit all 6 files (or just 3 if the submission script doesn’t have to change). If it’s just 3 files, it’s not too bad, but if I want to run 10 or 20 simulations with slight changes in the input file for each one, manual editing gets real tedious real fast. I got fed up with it and wrote a script to do all the editing for me. The script is called param-sweep.sh. Feel free to look at it on Bitbucket\nBefore running the script, I make a template for the input file and submission script with parameter names that the script will replace with parameter values. My input file template could look something like this:\n1000        # Number of iterations\n60          # Polymer length\n1           # Nanorod radius\nNRLENGTH    # Nanorod length\nand my submission script template could look something like this:\n#!/bin/sh\n#PBS -N TRIALNAME\n#PBS -l nodes=1:ppn=12\n#PBS -l walltime=01:00:00,mem=2gb\n\ncd $PBS_O_WORKDIR\n\n# Run code that looks for bcp.input in the current directory\nmpirun $HOME/code/awesome_code.exe\nIn this example, I want to replace NRLENGTH with the actual nanorod length for each bcp.input file in ./length1, ./length2, and ./length3, and I want to replace TRIALNAME with a name corresponding to each simulation in each sub.sh file. The script does this by looking through a trials.txt file I make that would look like this in this case:\nname        i:NRLENGTH  s:TRIALNAME\nlength1     4           length1-trial\nlength2     5           length2-trial\nlength3     6           length3-trial\nThe i: and s: before NRLENGTH and TRIALNAME, respectively, tell the script to look in the input file or submission script for each variable. Finally, let’s look at how to use the script:\n$ ls\nbcp.input   trials.txt  sub.sh\n$ ~/scripts/param-sweep.sh -t trials.txt -i bcp.input -s sub.sh\nTrials file:        trials.txt\nInput file:         bcp.input\nSubmission script:  sub.sh\n3 trials\n2 vars\nSubmitting trial length1:\n1443364.rrlogin.internal\nSubmitting trial length2:\n1443365.rrlogin.internal\nSubmitting trial length3:\n1443366.rrlogin.internal\n$ tree\n.\n├── bcp.input\n├── length1\n│   ├── bcp.input\n│   └── sub.sh\n├── length2\n│   ├── bcp.input\n│   └── sub.sh\n├── length3\n│   ├── bcp.input\n│   └── sub.sh\n├── sub.sh\n└── trials.txt\nSo the script made directories for all three simulations, replaced NRLENGTH with 4, 5, and 6 in the bcp.input files, replaced TRIALNAME with length1-trial, length2-trial, and length3-trial in the sub.sh files, and submitted the sub.sh files from within their respective simulation directories. In this case, since my script expects files with the names I used, I could have just typed ~/scripts/param-sweep.sh. If I wanted to be able to check the files before submitting, I could have typed ~/scripts/param-sweep.sh -n which would create the directories and files without submitting the jobs.\nA few caveats: the script isn’t currently set up to handle more than one layer of simulation directories. Also, the script as it’s set up right now copies whatever input file and submission script its fed to files named bcp.input and sub.sh. Finally, you’ll need to make sure that the variable name you want the script to find and replace with variable values doesn’t show up anywhere else in the file. The script will find and replace all instances of the variable name (case sensitive).\nThis script has saved me a lot of time. Hopefully it can help someone else out there too."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "",
    "text": "The Django Cookiecutter template is an amazing tool to start up a Django project with all the bells and whistles ready to go. Getting your production site up and running can still be a bit of a hassle though, so to save myself, and hopefully a few others, from this hassle in the future, I’m recording all the steps that worked for me here. I’m sure there are other ways of doing this, and I’d love feedback on how to simplify the process."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#quick-note",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#quick-note",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "Quick Note",
    "text": "Quick Note\nFollowing these steps may incur some small charges on AWS even if you think you’re on the “Free Tier”. Keep an eye on your billing dashboard and terminate all EC2 instances and delete Elastic IPs once you’re done with them. For reference though, my billing forecast shows $1.38 for the month of July after tons of messing around on AWS. If you have some AWS Educate credits, or some of your credits from starting an account left over, you should be fine though. Also you can set up billing alerts to make sure you don’t get caught off-guard."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#what-youll-get-out-of-this",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#what-youll-get-out-of-this",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "What you’ll get out of this",
    "text": "What you’ll get out of this\nIf you follow all these steps closely, by the end you will have an HTTPS enabled site with a custom domain name running via Docker on AWS EC2, backed by PostgreSQL, Redis, and Traefik. You’ll be able to create simple user profiles with email confirmations (with some caveats). And best of all, you won’t have spent a dime. This is all it will look like, but it’ll be ready for you to put it straight into production once you customize it.\n\n\n\nDjango Cookicutter Screenshot"
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#initialize-ec2-instance",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#initialize-ec2-instance",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "1. Initialize EC2 Instance",
    "text": "1. Initialize EC2 Instance\nLog in to your AWS Console (sign up for the Free Tier if you don’t have an account already). Click EC2 under services, and select your desired region from the menu near the top right of the page. I chose US East (Ohio) because that’s closest to my current location.\n\n\n\nRegion Selection Screenshot\n\n\nClick on Running Instances, then Launch Instance. This gives you a list of images you can pick from. I used the second one because it says it includes Python and Docker. Other images would probably work too.\n\n\n\nAWS AMI Selection Screenshot\n\n\nIn the launch wizard, the only thing you need to modify is the Security Group. Click on “6. Configure Security Group” at near the top of the screen, then click “Add Rule” to add HTTP, then do it again to add HTTPS. This is necessary to make your instance accessible as an HTTPS-enabled website.\n\n\n\nAWS EC2 Security Group Configuration\n\n\nWith that, you’re set to launch. When you click launch, you’ll see a screen asking you to select or create a key pair. I’ll create a new one called “test-aws” and hit “Download Key Pair”. A good place to store the test-aws.pem file is in your ~/.ssh/ folder. Whatever you do, do not version control this file. You don’t want to accidentally push your private key to Github.\n\n\n\nAWS Create Key Pair\n\n\nAfter downloading your key pair, make sure it has the correct permissions by running\nchmod 600 ~/.ssh/test-aws.pem\nMake sure to use the path that points to your .pem file if you named it differently or stored it somewhere else. Now you can finally launch your EC2 instance with the “Launch Instances” button. Now you can go back to your EC2 Dashboard, click on “Running Instances” and see your freshly launched EC2 instance."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#get-an-elastic-ip-address",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#get-an-elastic-ip-address",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "2. Get an Elastic IP Address",
    "text": "2. Get an Elastic IP Address\nYour EC2 Dashboard shows you a lot of info about your EC2 instance, including your IPv4 Public IP. This address could change during the lifetime of your EC2 instance though, so you need to assign it an Elastic IP, which will remain attached to your instance as long as you want. To do this, select the Elastic IP service in your AWS Console and click “Allocate new address”.\n\n\n\nAWS Allocate Elastic IP\n\n\nYou don’t have to change anything from the defaults. Going through that short wizard will give you a new Elastic IP address. In my case, it’s 3.19.20.222. Now that you have this IP Address, go to your Elastic IP Dashboard, select the new Elastic IP, click “Actions”, then “Associate address” to associate it with your running instance.\n\n\n\nAWS Associate Elastic IP\n\n\nIn the next screen, choose select your instance then click “Associate”.\n\n\n\nAWS Select Instance to Associate Elastic IP"
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#add-iam-role-to-ec2-instance",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#add-iam-role-to-ec2-instance",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "3. Add IAM Role to EC2 Instance",
    "text": "3. Add IAM Role to EC2 Instance\nThe last modification you have to do to your EC2 instance is to add an IAM role to it to allow it to access S3 storage, which is where the static files will be stored. First, we need to create the IAM role. Go to the IAM service in your AWS Console (search “IAM” in the search box). You’ll see something that looks like this with a few default IAM roles already available. We’ll make a new one that only has an S3 Access policy, so click “Create role”.\n\n\n\nAWS IAM Dashboad\n\n\nChoose the EC2 service, then click “Next: Permissions”.\n\n\n\nAWS Create IAM Role Page 1\n\n\nScroll down to the AmazonS3FullAccess policy, select that, then click “Next: Tags”.\n\n\n\nAWS Create IAM Role Page 2\n\n\nSkip to the next page, enter a name for this Role (I called it “S3Access”), and click “Create role”.\n\n\n\nAWS Create IAM Role Page 4\n\n\nNow we need to attach this role to our EC2 instance. Go to the Running Instances section of your EC2 Dashboard. Select your instance, then click “Actions > Instance Settings > Attach/Replace IAM Role”.\n\n\n\nAWS Attach IAM Role\n\n\nSelect your S3Access IAM role and attach it."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#create-s3-bucket",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#create-s3-bucket",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "4. Create S3 Bucket",
    "text": "4. Create S3 Bucket\nNow that our EC2 instance is all set up, we need to create an S3 bucket to store static files in. Go to the S3 service in your AWS Console and click “Create bucket”. In the wizard that pops up, make sure the Region is the same as your EC2 instance and pick a name for your bucket, then click “Next”.\n\n\n\nAWS Create S3 Bucket Page 1\n\n\nYou can stick with the default options in page 2 and click Next. On the “Set Permissions” page, you’ll need to uncheck all the boxes so it is publicly accessible.\n\n\n\nAWS Create S3 Bucket Page 3\n\n\nClick “Next”, then create the bucket."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#get-a-domain-name",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#get-a-domain-name",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "5. Get a Domain Name",
    "text": "5. Get a Domain Name\nTo enable HTTPS using Let’s Encrypt (the default certificate authority in the Django Cookiecutter template), you’ll need a domain name. You can get a domain name for free (!) from freenom.com. I got testaws.ga. Whether you use Freenom or some other service, you need to create A records that associate the Elastic IP we reserved for our EC2 instance with this domain name. It should look something like this, where the “Target” for both records is our Elastic IP:\n\n\n\nFreenom DNS Management\n\n\nBe aware that it might take a day or two for your IP address change to actually go through so that your domain name actually points to the right place."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#initialize-django-project-with-cookiecutter",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#initialize-django-project-with-cookiecutter",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "6. Initialize Django Project with Cookiecutter",
    "text": "6. Initialize Django Project with Cookiecutter\nOK, that was a lot of clicking through wizards so pat yourself on the back, stand up and stretch, maybe grab a coffee. Now we’re finally ready to start getting to Django. If you don’t already have cookiecutter installed (you can check with which cookiecutter), you can install it with\npip install cookiecutter\nOnce that’s installed, you can initialize your Django project with\ncookiecutter https://github.com/pydanny/cookiecutter-django\nThis asks a bunch of questions. Here’s how I answered them:\nYou've downloaded /Users/benlindsay/.cookiecutters/cookiecutter-django before. Is it okay to delete and re-download it? [yes]:\nproject_name [My Awesome Project]: Test Django On AWS\nproject_slug [test_django_on_aws]:\ndescription [Behold My Awesome Project!]: Deploying Django on AWS for Free\nauthor_name [Daniel Roy Greenfeld]: Ben Lindsay\ndomain_name [example.com]: testaws.ga\nemail [ben-lindsay@example.com]: benjlindsay@gmail.com\nversion [0.1.0]:\nSelect open_source_license:\n1 - MIT\n2 - BSD\n3 - GPLv3\n4 - Apache Software License 2.0\n5 - Not open source\nChoose from 1, 2, 3, 4, 5 (1, 2, 3, 4, 5) [1]:\ntimezone [UTC]: America/Chicago\nwindows [n]:\nuse_pycharm [n]:\nuse_docker [n]: y\nSelect postgresql_version:\n1 - 11.3\n2 - 10.8\n3 - 9.6\n4 - 9.5\n5 - 9.4\nChoose from 1, 2, 3, 4, 5 (1, 2, 3, 4, 5) [1]:\nSelect js_task_runner:\n1 - None\n2 - Gulp\nChoose from 1, 2 (1, 2) [1]:\nSelect cloud_provider:\n1 - AWS\n2 - GCP\n3 - None\nChoose from 1, 2, 3 (1, 2, 3) [1]:\ncustom_bootstrap_compilation [n]:\nuse_compressor [n]:\nuse_celery [n]:\nuse_mailhog [n]:\nuse_sentry [n]:\nuse_whitenoise [n]:\nuse_heroku [n]:\nuse_travisci [n]:\nkeep_local_envs_in_vcs [y]:\ndebug [n]:\n [SUCCESS]: Project initialized, keep up the good work!\nThe only options that really matter for our purposes are putting the correct domain name (testaws.ga in my case), choosing AWS as the cloud provider, and saying yes to use_docker. This makes a new folder called test_django_on_aws in my case with all the project files in it. Before doing anything else, let’s put our project in version control. The default .gitignore keeps production environment files out of version control, but always make sure you haven’t committed any secrets before pushing to Github.\ncd test_django_on_aws\ngit init\ngit add .\ngit cm -m \"files as generated by cookiecutter\"\nIf you have Docker and Docker Compose installed on your computer, you can test your Django project by running it locally. You don’t need Docker installed to deploy to AWS though. To run it locally, make sure you’re in the base project folder and run\ndocker-compose -f local.yml build\ndocker-compose -f local.yml up\nThe build command will take a long time the first time you run it so be patient. After running the up command, navigate to localhost:8000 in your browser, and you should see something like this, with a Django Debug Toolbar on the right.\n\n\n\nDjango Cookiecutter Local"
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#modify-production-environment-variables",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#modify-production-environment-variables",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "7. Modify Production Environment Variables",
    "text": "7. Modify Production Environment Variables\nBefore deploying to AWS, there are a couple production environment variables to add in. If you open up .envs/.production/.django, you’ll see a file with these lines in it:\n# Email\n# ------------------------------------------------------------------------------\nMAILGUN_API_KEY=\nDJANGO_SERVER_EMAIL=\nMAILGUN_DOMAIN=\n\n# AWS\n# ------------------------------------------------------------------------------\nDJANGO_AWS_ACCESS_KEY_ID=\nDJANGO_AWS_SECRET_ACCESS_KEY=\nDJANGO_AWS_STORAGE_BUCKET_NAME=\nAt a minimum, you need to add the S3 bucket name to DJANGO_AWS_STORAGE_BUCKET_NAME. If you want to be able to have users create accounts on the site, you’ll need to add a MAILGUN_API_KEY and MAILGUN_DOMAIN as well. If you sign up for a free account at mailgun.com, you can follow the instructions here to get your API key. My variables now look something like this:\n# Email\n# ------------------------------------------------------------------------------\nMAILGUN_API_KEY=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX-XXXXXXXX-XXXXXXXX\nDJANGO_SERVER_EMAIL=\nMAILGUN_DOMAIN=sandboxXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX.mailgun.org\n\n# AWS\n# ------------------------------------------------------------------------------\nDJANGO_AWS_ACCESS_KEY_ID=\nDJANGO_AWS_SECRET_ACCESS_KEY=\nDJANGO_AWS_STORAGE_BUCKET_NAME=test-aws-django-static"
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#install-and-start-docker-on-ec2-instance",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#install-and-start-docker-on-ec2-instance",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "8. Install and Start Docker on EC2 Instance",
    "text": "8. Install and Start Docker on EC2 Instance\nOK, time to actually start doing stuff on the EC2 instance. To ssh in, we need the hostname, which you can get from the Running Instances dashboard within the EC2 service. It should look something like ec2-WWW-XXX-YYY-ZZZ.us-east-2.compute.amazonaws.com, where WWW.XXX.YYY.ZZZ would be your public IP. To ssh into my instance, I run\nssh -i ~/.ssh/test-aws.pem ec2-user@ec2-3-19-20-222.us-east-2.compute.amazonaws.com\nNote: Your username is literally ec2-user, not something specific to you. It’s the same for everyone. Once you’re in, run a couple update/install commands:\nsudo yum update -y\nsudo yum install -y docker\nsudo service docker start\nsudo pip install docker-compose\nThis next one is a nice convenience command to make it so we don’t have to type sudo in front of all our docker commands.\nsudo usermod -aG docker ec2-user\nMake sure it’s working by typing docker ps which should give you this output:\n[ec2-user@ip-XXX-XXX-XXX-XXX ~]$ docker ps\nCONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES"
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#copy-files-to-ec2-instance",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#copy-files-to-ec2-instance",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "9. Copy Files to EC2 Instance",
    "text": "9. Copy Files to EC2 Instance\nOpen a new terminal window, and make sure you’re in your project’s top-level directory, i.e.\ncd /path/to/test_django_on_aws\nThen run this command with the right path to your .pem file and DNS:\nrsync -av -e \"ssh -i /path/to/your.pem\" . ec2-user@ec2-WWW-XXX-YYY-ZZZ.REGION.compute.amazonaws.com:~/app/\nNow in your terminal that’s ssh’ed into AWS, you should be able to\ncd ~/app\nand see all of your files copied there."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#build-and-deploy",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#build-and-deploy",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "10. Build and Deploy",
    "text": "10. Build and Deploy\nFinally, it’s time. Within your app directory on your EC2 instance, run the following two commands:\ndocker-compose -f production.yml build\ndocker-compose -f production.yml up -d\nThe build command will take a while, but the up command will be pretty fast. Now if all goes well, you should be able to navigate to your domain name in a browser (testaws.ga in my case) and see a screen like the very first screenshot in this blog post. If you don’t see it, keep calm and go to the next section. If you do, congratulations! The universe smiles kindly on you. Go purchase a lottery ticket, then skip to section 12 and read on about accessing the admin section and creating user profiles."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#it-didnt-work.-now-what",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#it-didnt-work.-now-what",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "11. It didn’t work. Now what?",
    "text": "11. It didn’t work. Now what?\nThere are so many moving parts that it’s pretty unlikely this will have worked perfectly the first time through. Getting to the point where I could write this post was a study in masochism, plowing through problem after problem until I finally got the web page to render. I’ll go through some of the problems I ran into and how I fixed them to give you some ideas about what to do next.\n\nThe domain might need some time to point to the right IP address. It can take up to a day or two. When Chrome couldn’t connect to the webpage, I opened a terminal and typed\n ping testaws.ga\nwhich responded with\n PING testaws.ga (3.17.199.231): 56 data bytes\n Request timeout for icmp_seq 0\n Request timeout for icmp_seq 1\nI had previously pointed testaws.ga at 3.17.199.231, so I double-checked to make sure I had it pointed at my new Elastic IP, 3.19.20.222 and…waited. ping kept showing the old IP for about an hour, so I let it sit overnight, and in the morning it pointed to the right one. Don’t worry if you get Request timeouts though. I get those and the site seems to be working fine.\nCheck the logs. Once I got to the point of running the docker-compose commands on the EC2 instance, checking the logs was crucial to debugging my problems. That (plus a fancy new website called Google.com) is how I found out that Let’s Encrypt doesn’t work without a domain name, how I found out I needed to point to an S3 bucket. The way you check them is to run\n docker-compose -f production.yml logs\nwithin the app directory of your EC2 instance. You’ll get a long printout of feedback from traefik, django, postgres, and redis. The main errors I ran into were traefik saying something like\n unable to generate a certificate for the domains [mydomain.blah]\nor a python stacktrace from django involving boto3, telling me there was a problem with S3. If S3 is working, you should see a folder called static in your S3 bucket in the AWS Console."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#admin-dashboard",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#admin-dashboard",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "12. Admin Dashboard",
    "text": "12. Admin Dashboard\nThe admin tool is a nice feature of Django, so let’s get that working. While sshed into your EC2 instance and in the app directory, first type\ndocker-compose -f production.yml run --rm django python manage.py migrate\nto run migrations that will make the admin panel available, then create a superuser with\ndocker-compose -f production.yml run --rm django python manage.py createsuperuser\nThe Django Cookiecutter template is very security-conscious, so it generated a random string as the URL for your admin page. Go to .envs/.production/.django and find the line that looks like\nDJANGO_ADMIN_URL=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/\nCopy that string and navigate to yourdomain.name/XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX/, and it should give you a login screen. Log in with the username and password you just created, and you should see this admin page:\n\n\n\nDjango Admin Page"
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#create-user-profiles",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#create-user-profiles",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "13. Create User Profiles",
    "text": "13. Create User Profiles\nBefore we try making a user profile, note that the free version of Mailgun only lets you send emails to addresses you have specified in your account. So to test this, you’ll need to go to app.mailgun.com/app/sending/domains and select your sandbox domain…\n\n\n\nMailgun Domains Page\n\n\n…then add whatever emails you want to the list of authorized recipients.\n\n\n\nMailgun Authorized Recipients\n\n\nMailgun will send a confirmation email to whatever addresses you list. Once you add and confirm an address, you can test out user profile creation on your Django site with that address. Django will send that a confirmation email to that address. For me, that email showed up in Spam at first, so check there if you don’t see it."
  },
  {
    "objectID": "posts/deploying-cookiecutter-django-site-on-aws/index.html#conclusion",
    "href": "posts/deploying-cookiecutter-django-site-on-aws/index.html#conclusion",
    "title": "Deploying a Cookiecutter Django Site on AWS",
    "section": "14. Conclusion",
    "text": "14. Conclusion\nI hope this post helps someone out there, especially future me. AWS can be a pain in the @$$ so hopefully having some detailed steps will make it just a little less painful. Happy coding, and I’d love to hear any feedback below or on the Twitters."
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html",
    "href": "posts/running-jupyter-lab-remotely/index.html",
    "title": "Running Jupyter Lab Remotely",
    "section": "",
    "text": "Updated Sept. 19, 2019: Added separate instructions for running on a compute node\nI’m a huge fan of Jupyter Notebooks, and I was very excited when I found out about Jupyter Lab, which provides a much more comprehensive user experience around Jupyter Notebooks. Other posts have covered in more detail why we should switch to using Jupyter Lab instead, so I won’t talk about that here.\nInstead, I just want to share how to run Jupyter Lab efficiently on a remote machine. I have a research cluster where I do most of my analyses for my PhD work, and running Jupyter Lab directly on the cluster means I don’t have to copy files between the cluster and my desktop.\nBefore we begin, one thing to keep in mind about research clusters, i.e. High Performance Computing (HPC) clusters, is the concept of a login node vs a compute node. When you ssh into your cluster, you are immediately in a login node, which is where you do all your main file editing and manipulation. These nodes usually don’t have the memory required for intense compute jobs, which is where the compute nodes come in. You typically submit jobs via job schedulers like SLURM or PBS to those compute nodes.\nWith that in mind, this post will be split into information for running Jupyter Lab on 1) a login node, and 2) a compute node. You should ask your system administrator which they would prefer.\nContents: - Running on a login node - Commands - Simplfying the remote side - Simplfying the local side - Putting it all together - Running on a compute node - Commands - Simplfying the remote side - Simplfying the local side - Putting it all together"
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html#commands",
    "href": "posts/running-jupyter-lab-remotely/index.html#commands",
    "title": "Running Jupyter Lab Remotely",
    "section": "Commands",
    "text": "Commands\nTo run Jupyter Lab on a login node, you need to open 2 terminal windows. In the first window:\n$ ssh username@hostname\n$ jupyter lab --no-browser --port=5678\n...\n[I 10:17:14.160 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 10:17:14.160 LabApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nNote that the default port for Jupyter is 8888, but to be courteous to other potential users of this cluster, you should choose an arbitrary 4-digit number (I arbitrarily chose 5678 in this case) to leave 8888 available in case someone else is playing with Jupyter.\nThen in the second window:\n$ ssh -CNL localhost:5678:localhost:5678 username@hostname\nThen in your web browser of choice, navigate to\nlocalhost:5678\nin the url bar. A login screen will show up asking for a token. Scroll down and you’ll see an option to set up a password:\n\n\n\nJupyter Password Setup\n\n\nEnter the token (i.e. XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX from above) and your desired password, and from now on you can just use your password to login.\nAll that is kind of a lot just to open up Jupyter Lab. So I found ways to significantly simplify the process from both the remote and local side."
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html#simplfying-the-remote-side",
    "href": "posts/running-jupyter-lab-remotely/index.html#simplfying-the-remote-side",
    "title": "Running Jupyter Lab Remotely",
    "section": "Simplfying the remote side",
    "text": "Simplfying the remote side\nTo make things easier on the remote machine side of things, tmux (or screen) and bash aliases and functions really come in handy. I like to have a Jupyter Lab session running constantly in my remote machine whether I’m logged in or not. Then I can ssh tunnel in to the existing session whenever I want! To do this, I do the following:\n$ ssh username@hostname\n$ tmux\n[ opens persistent shell session ]\n$ jlremote\n...\n[I 10:17:14.160 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 10:17:14.160 LabApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nI have jlremote defined as an alias in my remote ~/.bashrc file like so:\nalias jlremote='jupyter lab --no-browser --port=5678'\nSo once I have that Jupyter Lab session running, I can detach from the tmux session with CTRL-b, d (or CTRL-a, CTRL-d if you used the screen command), and let that process run indefinitely (days, weeks, months…).\nNow let’s deal with the local stuff."
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html#simplfying-the-local-side",
    "href": "posts/running-jupyter-lab-remotely/index.html#simplfying-the-local-side",
    "title": "Running Jupyter Lab Remotely",
    "section": "Simplfying the local side",
    "text": "Simplfying the local side\nOn the local side, I wanted to be able to run a single command like jllocal to open Jupyter Lab, so I wrote a bash function that goes in my local ~/.bashrc file. If you use this make sure to edit all the all-caps stuff, like USERNAME and HOSTNAME.\nfunction jllocal {\n  port=5678 \n  remote_username=USERNAME\n  remote_hostname=HOSTNAME\n  url=\"http://localhost:$port\" \n  echo \"Opening $url\"\n  open \"$url\"\n  cmd=\"ssh -CNL localhost:\"$port\":localhost:\"$port\" $remote_username@$remote_hostname\" \n  echo \"Running '$cmd'\"\n  eval \"$cmd\"\n}\nThis function does a few things when you type jllocal:\n\nRuns ssh tunneling command if it’s not already running\nGrabs the Jupyter token from the remote machine\nOpens a tab in your browser with the right url and token for you\n\nWhen you’re done accessing your Jupyter Lab session, type CTRL-C and it will shut down the ssh tunnel."
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html#putting-it-all-together",
    "href": "posts/running-jupyter-lab-remotely/index.html#putting-it-all-together",
    "title": "Running Jupyter Lab Remotely",
    "section": "Putting it all together",
    "text": "Putting it all together\nSo with an alias in place in your remote ~/.bashrc, a persistent remote tmux/screen session running Jupyter Lab, and a function defined in your local ~/.bashrc, all you need to do to open Jupyter Lab in your browser is a jllocal call on your local machine, and then CTRL-C when you’re done. It takes some initial set up work, but the simplicity in the end is worth it."
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html#commands-1",
    "href": "posts/running-jupyter-lab-remotely/index.html#commands-1",
    "title": "Running Jupyter Lab Remotely",
    "section": "Commands",
    "text": "Commands\nTo run Jupyter Lab on a compute node, you once again need to open 2 terminal windows. In the first window:\n$ ssh username@hostname\n$ srun --mem=2G --pty bash\n$ hostname\ncomputehostname\n$ jupyter lab --no-browser --port=5678 --ip=$(hostname)\n...\n[I 10:17:14.160 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 10:17:14.160 LabApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://computehostname:5678/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nIf that gives you any errors, especially one that says KeyError: 'runtime_dir' in it, try replacing the jupyter line with\n$ XDG_RUNTIME_DIR= jupyter lab --no-browser --port=5678 --ip=$(hostname)\nFor me, the XDG_RUNTIME_DIR environmental variable caused some problems, and setting it to a blank value fixed it.\nNote that the default port for Jupyter is 5678, but to be courteous to other potential users of this cluster, you should choose an arbitrary 4-digit number (I arbitrarily chose 5678 in this case) to leave 8888 available in case someone else is playing with Jupyter. Also, computehostname is whatever the node spits out when you type hostname. It could look something like node07.\nThen in the second window:\n$ ssh -CNL 5678:computehostname:5678 username@hostname\nWhere, once again, computehostname is the hostname of whatever compute node you got assigned when you ran your interactive compute job with the srun command above.\nThen in your web browser of choice, navigate to\nlocalhost:5678\nin the url bar. If you haven’t already set a password, a login screen will show up asking for a token. Scroll down and you’ll see an option to set up a password:\n\n\n\nJupyter Password Setup\n\n\nEnter the token (i.e. XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX from above) and your desired password, and from now on you can just use your password to login.\nOnce again, just like with running on the login node, these steps can be simplified."
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html#simplfying-the-remote-side-1",
    "href": "posts/running-jupyter-lab-remotely/index.html#simplfying-the-remote-side-1",
    "title": "Running Jupyter Lab Remotely",
    "section": "Simplfying the remote side",
    "text": "Simplfying the remote side\nHaving a forever Jupyter session on a compute node might not be an option for every cluster, but if it is, tmux or screen, plus bash aliases and functions once again come in handy. You can run an interactive compute job within a tmux session, then activate your Jupyter Lab session from within the interactive compute job, like this:\n$ ssh username@hostname\n$ tmux\n[ opens persistent shell session ]\n$ srun --mem=2G --pty bash\n$ jlremote\n...\n[I 10:17:14.160 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n[C 10:17:14.160 LabApp]\n\n    Copy/paste this URL into your browser when you connect for the first time,\n    to login with a token:\n        http://localhost:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nFor this case, I have jlremote defined a little differently than for running on the login node. Before, it was an alias. Now, it’s a function (still in my remote ~/.bashrc) that looks like this:\nfunction jlremote {\n    echo $(hostname) > ~/.jupyternode.txt\n    XDG_RUNTIME_DIR= jupyter lab --no-browser --port=9753 --ip=$(hostname)\n}\nThe main difference from before is that we’re creating a file with an arbitrary name that contains the compute node’s hostname. This will be used by the local machine to properly set up the ssh tunnel.\nSo once I have that Jupyter Lab session running, I can detach from the tmux session with CTRL-b, d (or CTRL-a, CTRL-d if you used the screen command), and let that process run indefinitely (days, weeks, months…), if your compute node allows it.\nNow let’s deal with the local stuff."
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html#simplfying-the-local-side-1",
    "href": "posts/running-jupyter-lab-remotely/index.html#simplfying-the-local-side-1",
    "title": "Running Jupyter Lab Remotely",
    "section": "Simplfying the local side",
    "text": "Simplfying the local side\nOn the local side, the jllocal bash function also has a few changes compared to the jllocal function for login node Jupyter sessions. The new function, which is still in my local ~/.bashrc file, looks like this, but with USERNAME and HOSTNAME changed to the proper values:\nfunction jllocal {\n    port=5678\n  remote_username=USERNAME\n  remote_hostname=HOSTNAME\n    node=$(ssh lindsb@rrlogin.seas.upenn.edu 'tail -1 ~/.jupyternode.txt')\n    url=\"http://localhost:$port\"\n    echo \"Opening $url\"\n    open \"$url\"\n    cmd=\"ssh -CNL \"$port\":\"$node\":\"$port\" $remote_username@$remote_hostname\"\n    echo \"Running '$cmd'\"\n    eval \"$cmd\"\n}\nThe main difference between this and the login node version of jllocal is that we grab the compute node hostname from the remote ~/.jupyternode.txt file we created, then use that in the ssh tunnel. Otherwise, it does all the same things. When you’re done accessing your Jupyter Lab session, type CTRL-C and it will shut down the ssh tunnel."
  },
  {
    "objectID": "posts/running-jupyter-lab-remotely/index.html#putting-it-all-together-1",
    "href": "posts/running-jupyter-lab-remotely/index.html#putting-it-all-together-1",
    "title": "Running Jupyter Lab Remotely",
    "section": "Putting it all together",
    "text": "Putting it all together\nJust like with the login node setup, with a function in place in your remote ~/.bashrc, a persistent remote tmux/screen session running Jupyter Lab, and a function defined in your local ~/.bashrc, ideally a single jllocal call on your local machine will open your browser and connect you to your session, then CTRL-C when you’re done. Hopefully all this helps someone out there, and feel free to ask questions below if you get stuck."
  }
]