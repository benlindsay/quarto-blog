<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ben Lindsay">
<meta name="dcterms.date" content="2019-10-14">
<meta name="description" content="I wanted to dive into the fundamentals of collaborative filtering and recommender systems, so I implemented a few common methods and compared them.">

<title>Shallow Learnings - Comparing Collaborative Filtering Methods</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Shallow Learnings</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About Me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://data-folks.masto.host/@benlindsay"><i class="bi bi-mastodon" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/benlindsay"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://linkedin.com/in/benjlindsay"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Comparing Collaborative Filtering Methods</h1>
  <div class="quarto-categories">
    <div class="quarto-category">recommender systems</div>
    <div class="quarto-category">collaborative filtering</div>
    <div class="quarto-category">python</div>
    <div class="quarto-category">numpy</div>
    <div class="quarto-category">pandas</div>
    <div class="quarto-category">seaborn</div>
    <div class="quarto-category">jupyter</div>
    <div class="quarto-category">matplotlib</div>
  </div>
  </div>

<div>
  <div class="description">
    I wanted to dive into the fundamentals of collaborative filtering and recommender systems, so I implemented a few common methods and compared them.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ben Lindsay </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 14, 2019</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>As part of a project sponsored by the data science team at <a href="https://www.airliquide.com/">Air Liquide</a> here in Philadelphia, I’m diving deep into collaborative filtering algorithms. There are 2 main goals of this project:</p>
<ol type="1">
<li>Gain understanding of a variety of collaborative filtering algorithms by implementing them myself</li>
<li>Compare quality and speed of a variety of algorithms as a function of dataset size</li>
</ol>
<p>The data I’m using comes from the <a href="https://grouplens.org/">GroupLens</a> research group, which has been collecting movie ratings from volunteers since 1995 and has curated datasets of a variety of sizes. For simplicity, I’ll focus on the 100K dataset, the smallest one, to enable faster iteration.</p>
<p>I split this project into several parts. Here’s a table of contents for you:</p>
<ul>
<li><a href="#exploratory-data-analysis">Exploratory Data Analysis</a></li>
<li><a href="#baseline-algorithms">Baseline Algorithms</a>
<ul>
<li><a href="#simple-average-model">Simple Average Model</a></li>
<li><a href="#average-by-id-model">Average By ID Model</a></li>
<li><a href="#damped-user--movie-baseline">Damped User + Movie Baseline</a></li>
<li><a href="#baseline-comparison">Baseline Comparison</a></li>
</ul></li>
<li><a href="#similarity-based-algorithms">Similarity-Based Algorithms</a></li>
<li><a href="#alternating-least-squares">Alternating Least Squares</a></li>
<li><a href="#stochastic-gradient-descent">Stochastic Gradient Descent</a></li>
<li><a href="#algorithm-comparisons">Algorithm Comparisons</a></li>
<li><a href="#recommender-system-prototype">Recommender System Prototype</a></li>
</ul>
<section id="exploratory-data-analysis" class="level2">
<h2 class="anchored" data-anchor-id="exploratory-data-analysis">Exploratory Data Analysis</h2>
<p><em>Check out the full notebook for this section</em> <em><a href="https://github.com/benlindsay/movielens-analysis/blob/master/01_Exploratory-Analysis-on-100K-data.ipynb">here</a>.</em></p>
<p>Before getting into any algorithm development, I wanted to get a picture of the data I was working with, so I asked the questions on my mind and tried to answer them with the data.</p>
<p>What does the ratings distribution look like?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-ratings-distribution.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Ratings Distribution</figcaption><p></p>
</figure>
</div>
<p>It’s a little skewed to the positive side, with 4 being the most common rating. I guess that skew makes sense because people are more likely to watch stuff they would like than stuff they would hate.</p>
<p>Next: how consistent are the ratings over time? If people as a whole get more positive or negative over time, that could complicate things. If their behavior doesn’t seem to change too much, we can make a simplifying assumption that time doesn’t matter and ignore time dependence.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-ratings-consistency.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Ratings Consistency</figcaption><p></p>
</figure>
</div>
<p>Looks pretty consistent, so we’re going to make that simplifying assumption. Purely out of curiosity, how much do the number of users and movies change over time?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-movie-and-user-count.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">User and Movie Count</figcaption><p></p>
</figure>
</div>
<p>The amount of growth in the short timespan of this dataset, particularly in the number of users, does make me think a more complicated approach could be warranted. Buuuuuut I don’t want to do that right now. We’ll stick with assuming we’re working with an <a href="https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">IID</a> dataset for the purposes of this project.</p>
<p>A very crucial aspect to understand about typical recommendation situations is the sparsity of your dataset. You want to predict how much every user likes every movie, but we have data about very few user-movie combinations. We’ll explore this in two ways.</p>
<p>First we’ll visualize the sparsity pattern of the user-movie matrix. This could be done with Matplotlib’s <a href="https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.spy.html">spy</a> function, but I didn’t know about it at the time I did this analysis, so I did this manually. The plot below shows a single, tiny black square for every user/movie combination we have. If everyone rated every movie, you’d see a solid black rectangle. Instead what we see is a lot of white–lots of user/movie combinations for which we don’t have a rating (yet). You especially see a lot of white in the top right corner. This is probably because early raters had access to fewer movies to rate, and new users progressively had more movies to rate as they were added to the system.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-sparsity-map.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">MovieLens Sparsity Map</figcaption><p></p>
</figure>
</div>
<p>The matrix density is <span class="math inline">\(n_{ratings}/(n_{users}×n_{movies})=0.063\)</span>, meaning that about 94% of the data we would like to know is missing.</p>
<p>In the plot above you also notice that there are a few darker rows and columns, but most rows and columns are pretty bare. Let’s visualize the distributions of number of ratings by user and by movie. The way I chose to visualize this is with an <a href="https://en.wikipedia.org/wiki/Empirical_distribution_function">Empirical Cumulative Distribution Function (ECDF)</a> plot. An ECDF plot has an advantage compared to a histogram that all data points can be plotted in a meaningful way, and no bin size has to be chosen to average arbitrary chunks of it. This is especially helpful with the long-tailed distributions here.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-ecdf.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ECDF Plot</figcaption><p></p>
</figure>
</div>
<p>In the plot above, you can learn, for example, that 40% of all users rated 50 or less movies, and 90% of movies have 169 or less ratings. In general, we seen that a large fraction of movies and users have few ratings associated with them, but a few movies and users have many more ratings.</p>
<p>The main thing to take from this though is that the matrix of possible ratings is quite sparse, and that we need to use models that deal with this lack of data.</p>
</section>
<section id="baseline-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="baseline-algorithms">Baseline Algorithms</h2>
<p><em>Check out the full notebook for this section</em> <em><a href="https://github.com/benlindsay/movielens-analysis/blob/master/02_Baselines.ipynb">here</a>.</em></p>
<p>Baseline models are important for 2 key reaons:</p>
<ol type="1">
<li>Baseline models give us a starting point to which to compare all future models, and</li>
<li>Smart baselines/averages may be needed to fill in missing data for more complicated models</li>
</ol>
<p>In this section, we’ll explore a few typical baseline models for recommender systems and see which ones do the best for our dataset. For all of these baseline models, and for that matter all the “real” models in the following sections, I coded them with the following structure, roughly similar to Scikit-learn’s API:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SomeBaselineModel():</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Run initialization steps</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fit(<span class="va">self</span>, X):</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute model parameters from ratings dataframe X with user, movie,</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="co"># and rating columns</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict(<span class="va">self</span>, X):</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Predict ratings for dataframe X with user and movie columns</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> predictions</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>I won’t actually put the code for all the models in here, but it’s all there in the Jupyter notebook.</p>
<section id="simple-average-model" class="level3">
<h3 class="anchored" data-anchor-id="simple-average-model">Simple Average Model</h3>
<p>The first model I implemented is about the simplest one possible, which I called <code>SimpleAverageModel</code>. We’ll average all the training set ratings and use that average for the prediction for all test set examples. It probably won’t do very well, but hey, it’s a baseline!</p>
</section>
<section id="average-by-id-model" class="level3">
<h3 class="anchored" data-anchor-id="average-by-id-model">Average By ID Model</h3>
<p>We can probably do a little better by using the user or item (movie) average. To do this, I set up a baseline model class, which I called <code>AverageByIdModel</code>, that allows you to pass either a list of <code>userId</code>s or <code>movieId</code>s as <code>X</code>. The prediction for a given ID will be the average of ratings from that ID, or the overall average if that ID wasn’t seen in the training set. This will probably get us a little farther than <code>SimpleAverageModel</code> but it still won’t win any million-dollar prizes.</p>
</section>
<section id="damped-user-movie-baseline" class="level3">
<h3 class="anchored" data-anchor-id="damped-user-movie-baseline">Damped User + Movie Baseline</h3>
<p>Lastly, we can likely do even better by taking into account average user <strong>and</strong> movie data for a given user-movie combo. It has an additional feature of a damping factor that can regularize the baseline prediction to prevent us from straying too far from that average of 4. The damping factor has been shown empirically to improve the baseline’s perfomance. I called my implementation <code>DampedUserMovieBaselineModel</code>.</p>
<p>This model follows equation 2.1 from a <a href="http://files.grouplens.org/papers/FnT%20CF%20Recsys%20Survey.pdf">collaborative filtering paper</a> from <a href="https://grouplens.org/">GroupLens</a>, the same group that published the MovieLens data. This equation defines rhe baseline rating for user <span class="math inline">\(u\)</span> and item <span class="math inline">\(i\)</span> as</p>
<p><span class="math display">\[b_{u,i} = \mu + b_u + b_i\]</span></p>
<p>where</p>
<p><span class="math display">\[b_u = \frac{1}{|I_u| + \beta_u}\sum_{i \in I_u} (r_{u,i} - \mu)\]</span></p>
<p>and</p>
<p><span class="math display">\[b_i = \frac{1}{|U_i| + \beta_i}\sum_{u \in U_i} (r_{u,i} - b_u - \mu).\]</span></p>
<p>(See equations 2.4 and 2.5). Here, <span class="math inline">\(\beta_u\)</span> and <span class="math inline">\(\beta_i\)</span> are damping factors, for which the paper reported 25 is a good number for this dataset. For now we’ll just leave these values equal (<span class="math inline">\(\beta=\beta_u=\beta_i\)</span>). Here’s a summary of the meanings of all the variables here:</p>
<table class="table">
<colgroup>
<col style="width: 22%">
<col style="width: 77%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Term</th>
<th style="text-align: left;">Meaning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(b_{u,i}\)</span></td>
<td style="text-align: left;">Baseline rating for user <span class="math inline">\(u\)</span> on item (movie) <span class="math inline">\(i\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mu\)</span></td>
<td style="text-align: left;">The mean of all ratings</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(b_u\)</span></td>
<td style="text-align: left;">The deviation from <span class="math inline">\(\mu\)</span> associated with user <span class="math inline">\(u\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(b_i\)</span></td>
<td style="text-align: left;">The deviation from <span class="math inline">\(\mu+b_u\)</span> associated with user <span class="math inline">\(i\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(I_u\)</span></td>
<td style="text-align: left;">The set of all items rated by user <span class="math inline">\(u\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mid I_u \mid\)</span></td>
<td style="text-align: left;">The number of items rated by user <span class="math inline">\(u\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\beta_u\)</span></td>
<td style="text-align: left;">Damping factor for the users (<span class="math inline">\(=\beta\)</span>)</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(r_{u,i}\)</span></td>
<td style="text-align: left;">Observed rating for user <span class="math inline">\(u\)</span> on item <span class="math inline">\(i\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(U_i\)</span></td>
<td style="text-align: left;">The set of all users who rated item <span class="math inline">\(i\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math inline">\(\mid U_i \mid\)</span></td>
<td style="text-align: left;">The number of users who rated item <span class="math inline">\(i\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math inline">\(\beta_i\)</span></td>
<td style="text-align: left;">Damping factor for the items (<span class="math inline">\(=\beta\)</span>)</td>
</tr>
</tbody>
</table>
</section>
<section id="baseline-comparison" class="level3">
<h3 class="anchored" data-anchor-id="baseline-comparison">Baseline Comparison</h3>
<p>With those baseline models defined, let’s compare them. In the plot below, I test 7 baseline models. The first is the <code>SimpleAverageModel</code>. The next two use the <code>AverageByIdModel</code> looking at averages by Item ID and User ID, respectively. The last 4 use the <code>DampedUserMovieBaseline</code> with different damping factors (<span class="math inline">\(\beta\)</span>). The top plot shows the Mean Absolute Error (MAE) of each fold after using 5-fold cross-validation. I chose MAE so as not to overly penalize more extreme ratings (compared to Mean Squared Error) from people angrily or over-excitedly selecting 1 or 5. The bottom plot shows the distributions of the corresponding residuals, meaning the difference between actual and predicted ratings.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-baseline-comparison.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Baseline Comparison</figcaption><p></p>
</figure>
</div>
<p>The MAE plots above show that the combined model with a damping factor of 0 or 10 performs the best, followed by the item average, then the user average. It makes sense that taking into account deviations from the mean due to both user and item would perform the best: there are more degrees of freedom (<span class="math inline">\(n_{users}+n_{movies}\)</span> to be exact) being taken into account for each baseline prediction. The same idea explains why the item average performs better than the user average: there are more items than users in this dataset, so averaging over items gives you <span class="math inline">\(n_{movies}\)</span> degrees of freedom, which is greater than the <span class="math inline">\(n_{users}\)</span> degrees of freedom for the user average. The residual plots underneath the MAE plot illustrate that taking into account more data pulls the density of the residuals closer to 0.</p>
<p>Before moving on to collaborative filtering models, we’ll want to choose which model to use as a baseline. Both the Combined 0 and Combined 10 models performed equally well, but we’ll choose the Combined 10 model, because a higher damping factor is effectively stronger regularization, which will prevent overfitting better than a damping factor of 0.</p>
</section>
</section>
<section id="similarity-based-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="similarity-based-algorithms">Similarity-Based Algorithms</h2>
<p><em>Check out the full notebook for this section <a href="https://github.com/benlindsay/movielens-analysis/blob/master/03_Similarity-based_CF.ipynb">here</a>.</em></p>
<p>Now that we’ve established some simple baseline models and demonstrated that the Damped User + Movie Baseline model is the best of the few we tested, let’s move on to some actual collaborative filtering models. Here, we’ll explore user-based and item-based collaborative filtering.</p>
<p><img src="collaborativeFiltering-960x540.jpg" class="img-fluid" alt="Collaborative Filtering Image"> <em>Image by <a href="http://www.salemmarafi.com/">Salem Marafi</a>, found at <a href="http://www.salemmarafi.com/wp-content/uploads/2014/04/collaborativeFiltering-960x540.jpg">salemmarafi.com</a></em></p>
<p>The idea of these methods is to predict unseen ratings by looking at how similar users rated a particular item, or by looking at how similar items were rated by a particular user. Both methods fall under the category of K-Nearest Neighbor (KNN) models, since ratings from the <span class="math inline">\(k\)</span> most similar users or items are combined for the prediction.</p>
<p>In the notebook linked above, I’ve implemented a class called <code>KNNRecommender</code> that can accept a <code>mode</code> parameter of either <code>'user'</code> or <code>'item'</code>. In the plot below, I use 5-fold cross-validation to measure the MAE of user- and item-based models as a function of <span class="math inline">\(k\)</span>. The green band represents the mean <span class="math inline">\(\pm\)</span> standard deviation of the best baseline method chosen from the previous section.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-knn-model-k-xval.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">KNN k cross validation</figcaption><p></p>
</figure>
</div>
<p>Here we can see that Item-based collaborative filtering outperforms User-based collaborative filtering for all <span class="math inline">\(k\)</span>. This occurs for the same reason that the Item average baseline performed better than the User average baseline: there are generally more ratings per item than there are ratings per user, since there are more users than movies. (This reverse is true for larger datasets like the <a href="https://grouplens.org/datasets/movielens/20m/">MovieLens 20M Dataset</a> where there are more users than movies.)</p>
<p>We also see that the best Item-based CF model occurs around <span class="math inline">\(k=10\)</span> while the best User-based CF model occurs around <span class="math inline">\(k=20\)</span>. We’ll keep these in mind when comparing models later.</p>
<p>Next, we’ll start looking at matrix factorization methods, beginning with Alternating Least Squares.</p>
</section>
<section id="alternating-least-squares" class="level2">
<h2 class="anchored" data-anchor-id="alternating-least-squares">Alternating Least Squares</h2>
<p><em>Check out the full notebook for this section <a href="https://github.com/benlindsay/movielens-analysis/blob/master/04_ALS.ipynb">here</a>.</em></p>
<p>Previously, I showed how to use similarity-based approaches that guess unknown user-movie-rating triplets by looking at either movies with a similar rating profile or users with a similar rating profile. These approaches leave a lot of data on the table though. Matrix factorization is a way to both take into account more data and perform some regularizing dimensionality reduction to help deal with the sparsity problem.</p>
<p>The basic idea is to organize the user-movie-rating triplets into a matrix with each row representing a user and each column representing a movie. We want to approximate this large matrix with a matrix multiplication of 2 smaller matrices. In the example below, each row of the “User Matrix” has 2 latent features of that user, and each column of the “Item Matrix” has 2 latent features of that item. The dot product of any user’s latent features and item’s latent features will give an estimate of the rating that user would give that movie.</p>
<p><img src="matrix-factorization.png" class="img-fluid" alt="Matrix Factorization"> <em>Image by <a href="https://medium.com/@connectwithghosh">Soumya Gosh</a>, found at <a href="https://medium.com/@connectwithghosh/simple-matrix-factorization-example-on-the-movielens-dataset-using-pyspark-9b7e3f567536">medium.com</a></em></p>
<p>There are many variations on this theme and multiple ways to perform this matrix factorization. The method I demonstrate here is called “Alternating Least Squares” method which was designed for the <a href="https://www.netflixprize.com/">Netflix Prize</a> and described in <a href="http://www.grappa.univ-lille3.fr/~mary/cours/stats/centrale/reco/paper/MatrixFactorizationALS.pdf">this paper</a>. This method works iteratively, with 2 main steps per iteration:</p>
<ol type="1">
<li>Assume the User Matrix is fixed and solve for the Item Matrix</li>
<li>Assume the Item Matrix is fixed and solve for the User Matrix</li>
</ol>
<p>In the notebook linked above, the full code for the <code>ALSRecommender</code> can be found.</p>
<p>Since this is an iterative method, I first checked the amount of iterations/epochs for an arbitrary number of latent features <span class="math inline">\(k\)</span> before the error curves start to plateau:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="als-epochs.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ALS Epochs</figcaption><p></p>
</figure>
</div>
<p>So it looks like 15 or 20 epochs should be enough for Test Error to start plateauing. So now let’s stick with 15 epochs and use cross-validation to select an optimal <span class="math inline">\(k\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="als-k-xval.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">ALS k Cross-Validation</figcaption><p></p>
</figure>
</div>
<p>It looks like we have a Test Error minimum around <span class="math inline">\(k=5\)</span>, so we’ll call that the winner for the ALS category.</p>
<p>Great, so now let’s move on to a different matrix factorization approach: stochastic gradient descent.</p>
</section>
<section id="stochastic-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
<p><em>Check out the full notebook for this section <a href="https://github.com/benlindsay/movielens-analysis/blob/master/05_SGD.ipynb">here</a>.</em></p>
<p>Previously, I showed how to do matrix factorization using Alternating Least Squares (ALS). Now we’ll attempt to factorize the matrix into the same mathematical form, but we’ll use a different technique to get there.</p>
<p>Derivation details that give us the update equations we need can be found <a href="https://blog.insightdatascience.com/explicit-matrix-factorization-als-sgd-and-all-that-jazz-b00e4d9b21ea#d42b">here</a>. I’ll just give the start and finish here.</p>
<p>We start with a loss function that looks like this:</p>
<p><span class="math display">\[
L = \sum_{u,i}(r_{ui} - \hat{r}_{ui})^2
  + \lambda_{b_u} \sum_u \lVert b_u \lVert^2
  + \lambda_{b_i} \sum_i \lVert b_i \lVert^2 \\
  + \lambda_{x_u} \sum_u \lVert \mathbf{x}_u \lVert^2
  + \lambda_{y_i} \sum_i \lVert \mathbf{y}_i \lVert^2
\]</span></p>
<p>The first term is a sum of squared errors on the predicted rating, while all the other terms are regularizing penalties on too high of values, tunable by the 4 <span class="math inline">\(\lambda\)</span> parameters. <span class="math inline">\(\hat{r}_{ui}\)</span>, the predicted rating for user <span class="math inline">\(u\)</span> on item <span class="math inline">\(i\)</span>, is given by</p>
<p><span class="math display">\[
\hat{r}_{ui} = \mu + b_u + b_i + \mathbf{x}_u^\top \cdot \mathbf{y}_i
\]</span></p>
<p>With this setup, we can iterate over ratings, compute the gradient in the loss function for that point with respect to each parameter <span class="math inline">\(b_u\)</span>, <span class="math inline">\(b_i\)</span>, <span class="math inline">\(\mathbf{x}_u\)</span>, and <span class="math inline">\(\mathbf{y}_i\)</span>. As mentioned in the post linked above, the final update equations look like this</p>
<p><span class="math display">\[
%% MathJax doesn't support multiline equations, so I'm using a hack to get them to
%% render correctly, pulled from
%% https://github.com/mathjax/MathJax/issues/2312#issuecomment-538185951
\displaylines{
b_u^{t+1} = b_u^{t} + \eta (e_{ui} - \lambda_{b_u})b_u \\
b_i^{t+1} = b_i^{t} + \eta (e_{ui} - \lambda_{b_i})b_i \\
\mathbf{x}_u^{t+1} = \mathbf{x}_u^{t} + \eta (e_{ui} \mathbf{y}_i - \lambda_{x_u} \mathbf{x}_u) \\
\mathbf{y}_i^{t+1} = \mathbf{y}_i^{t} + \eta (e_{ui} \mathbf{x}_u - \lambda_{y_i} \mathbf{y}_i) \\
}
\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate (a parameter that controls the speed of descent down the gradients) and <span class="math inline">\(e_{ui}\)</span> is the prediction error given by <span class="math inline">\(\hat{r}_{ui} - r_{ui}\)</span>.</p>
<p>The code for the <code>SGDRecommender</code> and the tuning of that model can be found in the notebook linked above.</p>
<p>First, just like we did with ALS, let’s see how the testing error changes as this iterative model progresses:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sgd-epochs.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">SGD Epochs</figcaption><p></p>
</figure>
</div>
<p>It looks like around 12 is the optimal number of iterations to run before we start overfitting, so we’ll use that from here on out. Next, just like before, let’s use cross-validation to find the best <span class="math inline">\(k\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="sgd-k-xval.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">SGD k Cross-Validation</figcaption><p></p>
</figure>
</div>
<p>Honestly, the fact that training error came back up at <span class="math inline">\(k=50\)</span> probably means I didn’t use the right amount of iterations/epochs, because training error should always go down with increasing model complexity. But my implementation of SGD is pretty slow and painful, and I really don’t want to rerun this. Using Cython or some other method to move the large amount of for looping into the C-layer could significantly reduce this pain, but I’m not getting into that right now.</p>
<p>With that caveat in mind, since <span class="math inline">\(k=50\)</span> resulted in the lowest test error, we’ll declare that the winner of the SGD variants and move on to comparing all the models.</p>
</section>
<section id="algorithm-comparisons" class="level2">
<h2 class="anchored" data-anchor-id="algorithm-comparisons">Algorithm Comparisons</h2>
<p><em>Check out the full notebook for this section <a href="https://github.com/benlindsay/movielens-analysis/blob/master/06_Model-Comparisons.ipynb">here</a>.</em></p>
<p>Now that I’ve implemented 3 main classes of collaborative filtering methods (similarity-based, alternating least squares (ALS), and stochastic gradient descent (SGD)), it’s time to see how they stack up to each other.</p>
<p>To compare models, I’ll use 2 different metrics: mean absolute error (MAE) and normalized discounted cumulative gain (NDCG). MAE measures about how many stars off all the predictions are on average. This is useful information, but in most recommendation situations, the user will only see a few of the top recommendations given to them. The NDCG score tells us how “good” the top few recommendations are, with decreasing weight given the farther you go down the list.</p>
<p>Usually, NDCG will be reported for a certain number of recommendations. If we just care about the first 3 recommendations, we would compute NDCG@3. If there were no movies that the user would have rated more highly than these 3, then NDCG@3 is 1.0. Lower values mean other movies would have gotten higher ratings.</p>
<p>If you’re interested, the math looks like this:</p>
<p>Given a vector <span class="math inline">\(\mathbf{r}\)</span> of <span class="math inline">\(k\)</span> recommendations from most to least recommended, discounted cumulative gain (DCG) is given by:</p>
<p><span class="math display">\[DCG@k = \sum_{i=1}^k \frac{r_i}{\log_2(i+1)}\]</span></p>
<p>Normalized DCG (NDCG) is DCG divided by the maximum possible DCG:</p>
<p><span class="math display">\[ NDCG@k = \frac{DCG@k}{\max_{\mathbf{r}} DCG@k}\]</span></p>
<p>First let’s choose the best User-based model:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mae-ndcg-user-k-xval.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">MAE and NDCG for User-based</figcaption><p></p>
</figure>
</div>
<p>NDCG@3 peaks at k=50, and MAE is pretty similar between k=20 to 100, so k=50 is the winner. Now let’s do the same thing for an item-based recommender:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mae-ndcg-item-k-xval.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">MAE and NDCG for Item-based</figcaption><p></p>
</figure>
</div>
<p>Here, <span class="math inline">\(k=10\)</span> and <span class="math inline">\(k=20\)</span> have similar MAE and NDCG@3, we’ll favor higher <span class="math inline">\(k\)</span> in nearest neigbor methods because higher <span class="math inline">\(k\)</span> is less prone to overfitting. <span class="math inline">\(k=20\)</span> is the winner of the item-based models.</p>
<p>Now with the iterative ALS and SGD models, we haven’t yet seen how NDCG@3 changes over time, so we need to examine that first before doing tuning the <span class="math inline">\(k\)</span> parameter.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mae-ndcg-als-epochs.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">MAE and NDCG vs Epoch for ALS-based</figcaption><p></p>
</figure>
</div>
<p>15 epochs still looks good for ALS, so let’s do our <span class="math inline">\(k\)</span> tuning, sticking with 15 iterations:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mae-ndcg-als-k-xval.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">MAE and NDCG vs k for ALS-based</figcaption><p></p>
</figure>
</div>
<p>Here, it looks like MAE is pretty flat with respect to the learning rate <span class="math inline">\(\lambda\)</span>, but NDCG@3 shows some interesting variations. The highest NDCG@3 comes from <span class="math inline">\(\lambda=0.1\)</span> and <span class="math inline">\(k&gt;=50\)</span>. With matrix factorization methods like ALS, we want to favor lower <span class="math inline">\(k\)</span> for better generalizability, so <span class="math inline">\(\lambda=0.1\)</span> and <span class="math inline">\(k=50\)</span> is the winner of the ALS category.</p>
<p>How does NDCG@3 change over time with the SGD model?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mae-ndcg-sgd-epochs.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">MAE and NDCG vs Epoch for SGD-based</figcaption><p></p>
</figure>
</div>
<p>Oof, looks like we’re going to need more than 15 epochs to get both the MAE and NDCG@3 to plateau, but I’m not redoing that plot because time is money and my slow implementation of SGD is sure costing a lot of time. I’ll ramp up to 30 iterations for model tuning hope that’s good enough. Now with SGD there are a lot more parameters you could tune. For the sake of time, we’ll stick with <span class="math inline">\(k=50\)</span> based on the ALS results, and tune the learning rate (<span class="math inline">\(\eta\)</span>) and regularization parameters (<span class="math inline">\(\lambda_*\)</span>). We’re going to further simplify things by forcing all the regularization parameters to be equal and call them <span class="math inline">\(\lambda\)</span>, i.e.</p>
<p><span class="math display">\[\lambda_{b_u}=\lambda_{b_i}=\lambda_{x_u}=\lambda_{y_i}=\lambda\]</span></p>
<p>Here’s are the errors and NDCG@3 as a function of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\eta\)</span>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="mae-ndcg-sgd-lambda-xval.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">MAE and NDCG for SGD model tuning</figcaption><p></p>
</figure>
</div>
<p><span class="math inline">\(\lambda=\eta=0.01\)</span> gives the best combination of MAE and NDCG@3, so that combination is the winner for SGD.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-model-comparison.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Model Comparison</figcaption><p></p>
</figure>
</div>
<p>There’s a lot of information in the 3 charts above. The charts show 3 different metrics (Mean Absolute Error, Normalized Discounted Cumulative Gain, and time) for the best user-based, item-based, ALS, and SGD models I found. Each metric/model combination has 3 points, representing the values for each of the 3 folds used for cross-validation.</p>
<p>The MAE doesn’t seem to change much across the different models, although the variance seems to be slightly smaller for the matrix factorization methods (ALS and SGD) compared to the nearest neighbors methods (user-based and item-based).</p>
<p>The NDCG@3 does seem to vary across the different models though, with the highest score going to the ALS model. NDCG@3 is arguably the more useful metric for a recommender system, so as long as very high speeds aren’t important, ALS wins here.</p>
<p>If this ALS model is too slow for a particular application, the item-based method would be the next choice. Both user- and item-based recommenders have similarly fast training speeds, with item-based having a slightly higher NDCG@3 score. The slower execution of the ALS and SGD models are likely related to the number of iterations over for loops required in each iteration.</p>
<p>As they are right now, my user- and item-based models don’t need any python for loops during training. ALS has <span class="math inline">\(n_{users} + n_{movies}\)</span> python for loop iterations per epoch, and SGD has <span class="math inline">\(n_{ratings}\)</span> iterations per epoch, which is about an order of magnitude higher. I specify “python” for loops, because the vectorized operations used in user-based, item-based, and ALS models have for loops in c which are much faster than those in python. By optimizing code with something like cython or numba, I could certainly drop the training time for ALS and SGD.</p>
</section>
<section id="recommender-system-prototype" class="level2">
<h2 class="anchored" data-anchor-id="recommender-system-prototype">Recommender System Prototype</h2>
<p>If you want to play with these models interactively, check out <a href="https://github.com/benlindsay/movielens-analysis/blob/master/07_Recommender.ipynb">my recommender notebook</a>. With this notebook, you could choose whichever user you want, show some of their favorite movies, then display the top recommendations given by any of these 4 models.</p>
<p>Below is a screenshot of what you’ll see if you use this notebook. You input a user id (user 30 in this case), and the notebook displays posters of 5 of that user’s most highly rated movies. You choose a model (<code>'als'</code> in this case–other options are <code>'user'</code>, <code>'item'</code>, or <code>'sgd'</code>), and the notebook takes a little time to compute, then displays the 3 movies the model most strongly recommends that user should watch. It’s not perfect, and I haven’t really verified that the results make sense, but it does something, so enjoy, and feel free to ask any questions below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-prototype-liked-movies.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Movies the input user likes</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="movielens-prototype-recommended-movies.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Movies the system recommends</figcaption><p></p>
</figure>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="benlindsay/quarto-blog" data-repo-id="R_kgDOIhZJDw" data-category="Announcements" data-category-id="DIC_kwDOIhZJD84CS1B6" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
</div> <!-- /content -->



</body></html>